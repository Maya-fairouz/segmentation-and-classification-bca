{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11526622,"sourceType":"datasetVersion","datasetId":7229200},{"sourceId":11593193,"sourceType":"datasetVersion","datasetId":7269824},{"sourceId":11623332,"sourceType":"datasetVersion","datasetId":7291826}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nibabel as nib\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nfrom torchvision import transforms, models\nimport os\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# Define paths to your dataset\ndata_dir = \"/kaggle/input/all-zendo-dataset/DATA/\"\nt2wi_dir = os.path.join(data_dir, \"T2WI\")\ncsv_path = \"/kaggle/input/all-zendo-dataset/DATA/all_centers_combined.csv\"\n\n# Load the CSV file\ndf = pd.read_csv(csv_path)\ndf['image_name'] = df['image_name'].str.replace('.nii.gz', '.nii')\nprint(\"CSV Columns:\", df.columns)\nprint(\"First few rows of CSV:\")\nprint(df.head())\n\n# Function to load a NIfTI file\ndef load_nifti(file_path):\n    nifti = nib.load(file_path)\n    return nifti.get_fdata()\n\n# Pair T2WI images with their labels from the CSV\nimage_label_pairs = []\nfor t2wi_subfolder in os.listdir(t2wi_dir):\n    subfolder_path = os.path.join(t2wi_dir, t2wi_subfolder)\n    if not os.path.isdir(subfolder_path):\n        continue\n    t2wi_files = [f for f in os.listdir(subfolder_path) if f.endswith(\".nii\")]\n    if t2wi_files:\n        t2wi_path = os.path.join(subfolder_path, t2wi_files[0])\n        t2wi_id = t2wi_subfolder.split(\".\")[0]\n        label_row = df[df['image_name'].str.replace('.nii', '') == t2wi_id]\n        if not label_row.empty:\n            label = label_row['label'].iloc[0]\n            image_label_pairs.append((t2wi_path, label))\n        else:\n            print(f\"No label found for T2WI ID {t2wi_id}\")\n\nprint(\"Paired T2WI-Label pairs:\", len(image_label_pairs))\nfor t2wi_path, label in image_label_pairs[:5]:\n    print(f\"T2WI: {t2wi_path}, Label: {label}\")\n\n# Compute class weights to handle imbalance\nlabels = [pair[1] for pair in image_label_pairs]\nnmbic_count = labels.count(0)\nmbic_count = labels.count(1)\ntotal = nmbic_count + mbic_count\nclass_weights = torch.tensor([total / (2 * nmbic_count), total / (2 * mbic_count)]).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\nprint(f\"Class Weights (NMBIC, MBIC): {class_weights}\")\n\n# Define data augmentation and transforms\ntrain_transform = A.Compose([\n    A.Resize(256, 256),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.Rotate(limit=30, p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.Normalize(mean=0.5, std=0.5),\n    ToTensorV2(),\n])\n\nval_transform = A.Compose([\n    A.Resize(256, 256),\n    A.Normalize(mean=0.5, std=0.5),\n    ToTensorV2(),\n])\n\n# Custom Dataset class for classification\nclass BladderCancerDataset(Dataset):\n    def __init__(self, image_label_pairs, transform=None):\n        self.image_label_pairs = image_label_pairs\n        self.transform = transform\n        self.slices = []\n        for idx, (img_path, label) in enumerate(self.image_label_pairs):\n            try:\n                img = load_nifti(img_path)\n                img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n                middle_slice = img.shape[2] // 2\n                self.slices.append((idx, middle_slice, img, label))\n            except Exception as e:\n                print(f\"Error loading {img_path}: {e}\")\n\n    def __len__(self):\n        return len(self.slices)\n\n    def __getitem__(self, idx):\n        img_idx, z, img, label = self.slices[idx]\n        img_slice = img[:, :, z]\n        if self.transform:\n            augmented = self.transform(image=img_slice)\n            img_slice = augmented['image']\n        else:\n            img_slice = torch.FloatTensor(img_slice).unsqueeze(0)\n        label = torch.LongTensor([label])\n        return img_slice, label\n\n    def get_pair_index(self, idx):\n        return self.slices[idx][0]\n\n# Create datasets with transforms\ntrain_dataset = BladderCancerDataset(image_label_pairs, transform=train_transform)\nval_dataset = BladderCancerDataset(image_label_pairs, transform=val_transform)\n\n# Split at the pair level\npair_indices = list(range(len(image_label_pairs)))\ntrain_pair_indices, val_pair_indices = train_test_split(pair_indices, test_size=0.2, random_state=42)\ntrain_indices, val_indices = [], []\nfor slice_idx in range(len(train_dataset)):\n    pair_idx = train_dataset.get_pair_index(slice_idx)\n    if pair_idx in train_pair_indices:\n        train_indices.append(slice_idx)\n    elif pair_idx in val_pair_indices:\n        val_indices.append(slice_idx)\n\n# Create data loaders\ntrain_sampler = SubsetRandomSampler(train_indices)\nval_sampler = SubsetRandomSampler(val_indices)\ntrain_loader = DataLoader(train_dataset, batch_size=8, sampler=train_sampler)\nval_loader = DataLoader(val_dataset, batch_size=8, sampler=val_sampler)\n\n# Use a pretrained ResNet50 model\nmodel = models.resnet50(pretrained=True)\nmodel.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\nmodel.fc = nn.Sequential(\n    nn.Linear(model.fc.in_features, 512),\n    nn.ReLU(),\n    nn.Dropout(0.6),\n    nn.Linear(512, 2)\n)\n\n# Move model to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Define loss, optimizer, and scheduler with weight decay\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n\n# Training loop with validation and early stopping\nnum_epochs = 100\ntrain_losses, val_losses = [], []\ntrain_accuracies, val_accuracies = [], []\nbest_val_loss = float('inf')\npatience = 10\nearly_stop_counter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    train_correct = 0\n    train_total = 0\n    for images_batch, labels_batch in train_loader:\n        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device).squeeze(1)\n        outputs = model(images_batch)\n        loss = criterion(outputs, labels_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        train_total += labels_batch.size(0)\n        train_correct += (predicted == labels_batch).sum().item()\n    train_loss /= len(train_loader)\n    train_accuracy = train_correct / train_total\n    train_losses.append(train_loss)\n    train_accuracies.append(train_accuracy)\n\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for images_batch, labels_batch in val_loader:\n            images_batch, labels_batch = images_batch.to(device), labels_batch.to(device).squeeze(1)\n            outputs = model(images_batch)\n            loss = criterion(outputs, labels_batch)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            val_total += labels_batch.size(0)\n            val_correct += (predicted == labels_batch).sum().item()\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels_batch.cpu().numpy())\n    val_loss /= len(val_loader)\n    val_accuracy = val_correct / val_total\n    val_losses.append(val_loss)\n    val_accuracies.append(val_accuracy)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n    \n    scheduler.step(val_loss)\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        early_stop_counter = 0\n        torch.save(model.state_dict(), \"best_model.pth\")\n    else:\n        early_stop_counter += 1\n        if early_stop_counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\n# Load the best model\nmodel.load_state_dict(torch.load(\"best_model.pth\"))\n\n# Plot training and validation accuracy/loss\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\nplt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Val Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.grid()\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\nplt.plot(range(1, len(val_losses) + 1), val_losses, label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.grid()\nplt.tight_layout()\nplt.show()\n\n# Generate and plot confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(6, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['NMBIC', 'MBIC'], yticklabels=['NMBIC', 'MBIC'])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Visualize predictions on validation set\nmodel.eval()\nval_iter = iter(val_loader)\nimages, labels = next(val_iter)\nimages, labels = images.to(device), labels.to(device).squeeze(1)\nwith torch.no_grad():\n    outputs = model(images)\n    _, preds = torch.max(outputs, 1)\nimages, labels, preds = images.cpu(), labels.cpu(), preds.cpu()\nnum_samples = min(15, len(images))\nplt.figure(figsize=(15, 4 * num_samples))\nfor i in range(num_samples):\n    plt.subplot(num_samples, 2, 2 * i + 1)\n    plt.imshow(images[i, 0], cmap=\"gray\")\n    plt.title(f\"Label: {'MBIC' if labels[i] == 1 else 'NMBIC'}\")\n    plt.axis(\"off\")\n    plt.subplot(num_samples, 2, 2 * i + 2)\n    plt.imshow(images[i, 0], cmap=\"gray\")\n    plt.title(f\"Predicted: {'MBIC' if preds[i] == 1 else 'NMBIC'}\")\n    plt.axis(\"off\")\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nibabel as nib\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nimport os\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom monai.transforms import Compose, LoadImaged, Resize, RandFlipd, RandRotate90d, NormalizeIntensityd, ToTensord\nfrom monai.networks.nets import BasicUNet\nfrom monai.data import Dataset as MonaiDataset\n\n# Define paths to your dataset\ndata_dir = \"/kaggle/input/all-zendo-dataset/DATA/\"\nt2wi_dir = os.path.join(data_dir, \"T2WI\")\ncsv_path = \"/kaggle/input/all-zendo-dataset/DATA/all_centers_combined.csv\"\n\n# Load the CSV file\ndf = pd.read_csv(csv_path)\ndf['image_name'] = df['image_name'].str.replace('.nii.gz', '.nii')\nprint(\"CSV Columns:\", df.columns)\nprint(\"First few rows of CSV:\")\nprint(df.head())\n\n# Function to load a NIfTI file\ndef load_nifti(file_path):\n    nifti = nib.load(file_path)\n    return nifti.get_fdata()\n\n# Pair T2WI images with their labels from the CSV\nimage_label_pairs = []\nfor t2wi_subfolder in os.listdir(t2wi_dir):\n    subfolder_path = os.path.join(t2wi_dir, t2wi_subfolder)\n    if not os.path.isdir(subfolder_path):\n        continue\n    t2wi_files = [f for f in os.listdir(subfolder_path) if f.endswith(\".nii\")]\n    if t2wi_files:\n        t2wi_path = os.path.join(subfolder_path, t2wi_files[0])\n        t2wi_id = t2wi_subfolder.split(\".\")[0]\n        label_row = df[df['image_name'].str.replace('.nii', '') == t2wi_id]\n        if not label_row.empty:\n            label = label_row['label'].iloc[0]\n            image_label_pairs.append((t2wi_path, label))\n        else:\n            print(f\"No label found for T2WI ID {t2wi_id}\")\n\nprint(\"Paired T2WI-Label pairs:\", len(image_label_pairs))\nfor t2wi_path, label in image_label_pairs[:5]:\n    print(f\"T2WI: {t2wi_path}, Label: {label}\")\n\n# Compute class weights to handle imbalance\nlabels = [pair[1] for pair in image_label_pairs]\nnmbic_count = labels.count(0)\nmbic_count = labels.count(1)\ntotal = nmbic_count + mbic_count\nclass_weights = torch.tensor([total / (2 * nmbic_count), total / (2 * mbic_count)]).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\nprint(f\"Class Weights (NMBIC, MBIC): {class_weights}\")\n\n# Define 3D data augmentation and transforms using MONAI\ntrain_transforms = Compose([\n    LoadImaged(keys=[\"image\"]),\n    Resize(spatial_size=(64, 64, 64), keys=[\"image\"]),\n    RandFlipd(keys=[\"image\"], prob=0.5, spatial_axis=0),\n    RandFlipd(keys=[\"image\"], prob=0.5, spatial_axis=1),\n    RandRotate90d(keys=[\"image\"], prob=0.5, max_k=3),\n    NormalizeIntensityd(keys=[\"image\"]),\n    ToTensord(keys=[\"image\", \"label\"]),\n])\n\nval_transforms = Compose([\n    LoadImaged(keys=[\"image\"]),\n    Resize(spatial_size=(64, 64, 64), keys=[\"image\"]),\n    NormalizeIntensityd(keys=[\"image\"]),\n    ToTensord(keys=[\"image\", \"label\"]),\n])\n\n# Custom Dataset class for 3D classification\nclass BladderCancerDataset(MonaiDataset):\n    def __init__(self, image_label_pairs, transforms=None):\n        self.data = [{\"image\": img_path, \"label\": label} for img_path, label in image_label_pairs]\n        super().__init__(data=self.data, transform=transforms)\n\n# Create datasets with transforms\ntrain_dataset = BladderCancerDataset(image_label_pairs, transforms=train_transforms)\nval_dataset = BladderCancerDataset(image_label_pairs, transforms=val_transforms)\n\n# Split at the pair level\npair_indices = list(range(len(image_label_pairs)))\ntrain_pair_indices, val_pair_indices = train_test_split(pair_indices, test_size=0.2, random_state=42)\ntrain_sampler = SubsetRandomSampler(train_pair_indices)\nval_sampler = SubsetRandomSampler(val_pair_indices)\n\n# Create data loaders with small batch size\ntrain_loader = DataLoader(train_dataset, batch_size=2, sampler=train_sampler, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=2, sampler=val_sampler, num_workers=0)\n\n# Define a lightweight 3D CNN model\nclass Simple3DCNN(nn.Module):\n    def __init__(self, num_classes=2):\n        super(Simple3DCNN, self).__init__()\n        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, padding=1)\n        self.relu = nn.ReLU()\n        self.pool = nn.MaxPool3d(2)\n        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n        self.global_pool = nn.AdaptiveAvgPool3d(1)\n        self.fc1 = nn.Linear(64, 128)\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))\n        x = self.pool(self.relu(self.conv2(x)))\n        x = self.pool(self.relu(self.conv3(x)))\n        x = self.global_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n# Initialize model\nmodel = Simple3DCNN(num_classes=2)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Define loss, optimizer, and scheduler\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n\n# Gradient accumulation for memory efficiency\naccumulation_steps = 4\n\n# Training loop with validation and early stopping\nnum_epochs = 20  # Reduced for resource efficiency\ntrain_losses, val_losses = [], []\ntrain_accuracies, val_accuracies = [], []\nbest_val_loss = float('inf')\npatience = 5\nearly_stop_counter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    train_correct = 0\n    train_total = 0\n    optimizer.zero_grad()\n    for i, batch in enumerate(train_loader):\n        images = batch[\"image\"].to(device)\n        labels = batch[\"label\"].to(device).squeeze()\n        outputs = model(images)\n        loss = criterion(outputs, labels) / accumulation_steps\n        loss.backward()\n        train_loss += loss.item() * accumulation_steps\n        if (i + 1) % accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n        _, predicted = torch.max(outputs, 1)\n        train_total += labels.size(0)\n        train_correct += (predicted == labels).sum().item()\n    train_loss /= len(train_loader)\n    train_accuracy = train_correct / train_total\n    train_losses.append(train_loss)\n    train_accuracies.append(train_accuracy)\n\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in val_loader:\n            images = batch[\"image\"].to(device)\n            labels = batch[\"label\"].to(device).squeeze()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    val_loss /= len(val_loader)\n    val_accuracy = val_correct / val_total\n    val_losses.append(val_loss)\n    val_accuracies.append(val_accuracy)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n    \n    scheduler.step(val_loss)\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        early_stop_counter = 0\n        torch.save(model.state_dict(), \"best_model.pth\")\n    else:\n        early_stop_counter += 1\n        if early_stop_counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\n# Load the best model\nmodel.load_state_dict(torch.load(\"best_model.pth\"))\n\n# Plot training and validation accuracy/loss\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\nplt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Val Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.grid()\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\nplt.plot(range(1, len(val_losses) + 1), val_losses, label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.grid()\nplt.savefig('training_plots.png')\nplt.close()\n\n# Generate and plot confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(6, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['NMBIC', 'MBIC'], yticklabels=['NMBIC', 'MBIC'])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.savefig('confusion_matrix.png')\nplt.close()\n\n# Visualize predictions on validation set (middle slice of 3D volume)\nmodel.eval()\nval_iter = iter(val_loader)\nbatch = next(val_iter)\nimages = batch[\"image\"].to(device)\nlabels = batch[\"label\"].to(device).squeeze()\nwith torch.no_grad():\n    outputs = model(images)\n    _, preds = torch.max(outputs, 1)\nimages, labels, preds = images.cpu(), labels.cpu(), preds.cpu()\nnum_samples = min(4, len(images))  # Reduced for resource efficiency\nplt.figure(figsize=(15, 4 * num_samples))\nfor i in range(num_samples):\n    middle_slice = images[i, 0, :, :, images.shape[4] // 2]  # Middle slice along z-axis\n    plt.subplot(num_samples, 2, 2 * i + 1)\n    plt.imshow(middle_slice, cmap=\"gray\")\n    plt.title(f\"Label: {'MBIC' if labels[i] == 1 else 'NMBIC'}\")\n    plt.axis(\"off\")\n    plt.subplot(num_samples, 2, 2 * i + 2)\n    plt.imshow(middle_slice, cmap=\"gray\")\n    plt.title(f\"Predicted: {'MBIC' if preds[i] == 1 else 'NMBIC'}\")\n    plt.axis(\"off\")\nplt.tight_layout()\nplt.savefig('predictions.png')\nplt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:06:03.603220Z","iopub.execute_input":"2025-05-01T23:06:03.603535Z","execution_failed":"2025-05-01T23:08:33.889Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: monai in /usr/local/lib/python3.11/dist-packages (1.4.0)\nRequirement already satisfied: numpy<2.0,>=1.24 in /usr/local/lib/python3.11/dist-packages (from monai) (1.26.4)\nRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from monai) (2.5.1+cu124)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9->monai) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->monai) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0,>=1.24->monai) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0,>=1.24->monai) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0,>=1.24->monai) (2024.2.0)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nimport keras\nfrom keras import layers\nimport nibabel as nib\nfrom scipy import ndimage\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Define paths to your dataset\ndata_dir = \"/kaggle/input/all-zendo-dataset/DATA/\"\nt2wi_dir = os.path.join(data_dir, \"T2WI\")\ncsv_path = \"/kaggle/input/all-zendo-dataset/DATA/all_centers_combined.csv\"\n\n# Load the CSV file\ndf = pd.read_csv(csv_path)\ndf['image_name'] = df['image_name'].str.replace('.nii.gz', '.nii')\n\n# Helper functions for preprocessing\ndef read_nifti_file(filepath):\n    \"\"\"Read and load volume\"\"\"\n    scan = nib.load(filepath)\n    scan = scan.get_fdata()\n    return scan\n\ndef normalize(volume):\n    \"\"\"Normalize the volume\"\"\"\n    min_hu = -1000\n    max_hu = 400\n    volume[volume < min_hu] = min_hu\n    volume[volume > max_hu] = max_hu\n    volume = (volume - min_hu) / (max_hu - min_hu)\n    volume = volume.astype(\"float32\")\n    return volume\n\ndef resize_volume(img):\n    \"\"\"Resize across z-axis\"\"\"\n    desired_depth = 64\n    desired_width = 128\n    desired_height = 128\n    current_depth = img.shape[-1]\n    current_width = img.shape[0]\n    current_height = img.shape[1]\n    depth = current_depth / desired_depth\n    width = current_width / desired_width\n    height = current_height / desired_height\n    depth_factor = 1 / depth\n    width_factor = 1 / width\n    height_factor = 1 / height\n    img = ndimage.rotate(img, 90, reshape=False)\n    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n    return img\n\ndef process_scan(path):\n    \"\"\"Read and resize volume\"\"\"\n    volume = read_nifti_file(path)\n    volume = normalize(volume)\n    volume = resize_volume(volume)\n    return volume\n\n# Load and pair T2WI images with labels\nnmbic_scan_paths = []\nmbic_scan_paths = []\nfor t2wi_subfolder in os.listdir(t2wi_dir):\n    subfolder_path = os.path.join(t2wi_dir, t2wi_subfolder)\n    if not os.path.isdir(subfolder_path):\n        continue\n    t2wi_files = [f for f in os.listdir(subfolder_path) if f.endswith(\".nii\")]\n    if t2wi_files:\n        t2wi_path = os.path.join(subfolder_path, t2wi_files[0])\n        t2wi_id = t2wi_subfolder.split(\".\")[0]\n        label_row = df[df['image_name'].str.replace('.nii', '') == t2wi_id]\n        if not label_row.empty:\n            label = label_row['label'].iloc[0]\n            if label == 0:\n                nmbic_scan_paths.append(t2wi_path)\n            else:\n                mbic_scan_paths.append(t2wi_path)\n        else:\n            print(f\"No label found for T2WI ID {t2wi_id}\")\n\nprint(f\"NMBIC scans: {len(nmbic_scan_paths)}\")\nprint(f\"MBIC scans: {len(mbic_scan_paths)}\")\n\n# Process scans\nnmbic_scans = np.array([process_scan(path) for path in nmbic_scan_paths])\nmbic_scans = np.array([process_scan(path) for path in mbic_scan_paths])\n\n# Assign labels (0 for NMBIC, 1 for MBIC)\nnmbic_labels = np.array([0 for _ in range(len(nmbic_scans))])\nmbic_labels = np.array([1 for _ in range(len(mbic_scans))])\n\n# Split data into training and validation (70-30)\nx_train = np.concatenate((mbic_scans[:int(0.7 * len(mbic_scans))], nmbic_scans[:int(0.7 * len(nmbic_scans))]), axis=0)\ny_train = np.concatenate((mbic_labels[:int(0.7 * len(mbic_labels))], nmbic_labels[:int(0.7 * len(nmbic_labels))]), axis=0)\nx_val = np.concatenate((mbic_scans[int(0.7 * len(mbic_scans)):], nmbic_scans[int(0.7 * len(nmbic_scans)):]), axis=0)\ny_val = np.concatenate((mbic_labels[int(0.7 * len(mbic_labels)):], nmbic_labels[int(0.7 * len(nmbic_labels)):]), axis=0)\n\nprint(f\"Number of samples in train and validation are {x_train.shape[0]} and {x_val.shape[0]}.\")\n\n# Data augmentation\ndef rotate(volume):\n    \"\"\"Rotate the volume by a few degrees\"\"\"\n    def scipy_rotate(volume):\n        angles = [-20, -10, -5, 5, 10, 20]\n        angle = np.random.choice(angles)\n        volume = ndimage.rotate(volume, angle, reshape=False)\n        volume[volume < 0] = 0\n        volume[volume > 1] = 1\n        return volume\n    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n    return augmented_volume\n\ndef train_preprocessing(volume, label):\n    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n    volume = rotate(volume)\n    volume = tf.expand_dims(volume, axis=3)\n    return volume, label\n\ndef validation_preprocessing(volume, label):\n    \"\"\"Process validation data by only adding a channel.\"\"\"\n    volume = tf.expand_dims(volume, axis=3)\n    return volume, label\n\n# Define data loaders\ntrain_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\nvalidation_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n\nbatch_size = 2\ntrain_dataset = (\n    train_loader.shuffle(len(x_train))\n    .map(train_preprocessing, num_parallel_calls=tf.data.AUTOTUNE)\n    .batch(batch_size)\n    .prefetch(2)\n)\nvalidation_dataset = (\n    validation_loader.shuffle(len(x_val))\n    .map(validation_preprocessing, num_parallel_calls=tf.data.AUTOTUNE)\n    .batch(batch_size)\n    .prefetch(2)\n)\n\n# Visualize an augmented CT scan\ndata = train_dataset.take(1)\nimages, labels = list(data)[0]\nimages = images.numpy()\nimage = images[0]\nprint(\"Dimension of the MRI scan is:\", image.shape)\nplt.figure()\nplt.imshow(np.squeeze(image[:, :, 30]), cmap=\"gray\")\nplt.savefig('sample_mri_scan.png')\nplt.close()\n\n# Visualize montage of slices\ndef plot_slices(num_rows, num_columns, width, height, data):\n    \"\"\"Plot a montage of MRI slices\"\"\"\n    data = np.rot90(np.array(data))\n    data = np.transpose(data)\n    data = np.reshape(data, (num_rows, num_columns, width, height))\n    rows_data, columns_data = data.shape[0], data.shape[1]\n    heights = [slc[0].shape[0] for slc in data]\n    widths = [slc.shape[1] for slc in data[0]]\n    fig_width = 12.0\n    fig_height = fig_width * sum(heights) / sum(widths)\n    f, axarr = plt.subplots(\n        rows_data,\n        columns_data,\n        figsize=(fig_width, fig_height),\n        gridspec_kw={\"height_ratios\": heights},\n    )\n    for i in range(rows_data):\n        for j in range(columns_data):\n            axarr[i, j].imshow(data[i][j], cmap=\"gray\")\n            axarr[i, j].axis(\"off\")\n    plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n    plt.savefig('mri_slices_montage.png')\n    plt.close()\n\nplot_slices(4, 10, 128, 128, image[:, :, :40])\n\n# Define 3D CNN model\ndef get_model(width=128, height=128, depth=64):\n    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n    inputs = keras.Input((width, height, depth, 1))\n    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.GlobalAveragePooling3D()(x)\n    x = layers.Dense(units=512, activation=\"relu\")(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n    return model\n\n# Build and compile model\nmodel = get_model(width=128, height=128, depth=64)\ninitial_learning_rate = 0.0001\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n)\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n    metrics=[\"acc\"],\n    run_eagerly=True,\n)\n\n# Define callbacks\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\n    \"3d_bladder_classification.keras\", save_best_only=True\n)\nearly_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n\n# Train the model\nepochs = 100\nhistory = model.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    epochs=epochs,\n    shuffle=True,\n    verbose=2,\n    callbacks=[checkpoint_cb, early_stopping_cb],\n)\n\n# Plot model performance\nfig, ax = plt.subplots(1, 2, figsize=(20, 3))\nax = ax.ravel()\nfor i, metric in enumerate([\"acc\", \"loss\"]):\n    ax[i].plot(history.history[metric])\n    ax[i].plot(history.history[\"val_\" + metric])\n    ax[i].set_title(f\"Model {metric}\")\n    ax[i].set_xlabel(\"epochs\")\n    ax[i].set_ylabel(metric)\n    ax[i].legend([\"train\", \"val\"])\nplt.savefig('model_performance.png')\nplt.close()\n\n# Make predictions on a single MRI scan\nmodel.load_weights(\"3d_bladder_classification.keras\")\nprediction = model.predict(np.expand_dims(x_val[0], axis=0))[0]\nscores = [1 - prediction[0], prediction[0]]\nclass_names = [\"NMBIC\", \"MBIC\"]\nfor score, name in zip(scores, class_names):\n    print(f\"This model is {100 * score:.2f} percent confident that MRI scan is {name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T17:44:14.160314Z","iopub.execute_input":"2025-05-02T17:44:14.160978Z","iopub.status.idle":"2025-05-02T17:56:45.455161Z","shell.execute_reply.started":"2025-05-02T17:44:14.160945Z","shell.execute_reply":"2025-05-02T17:56:45.454548Z"}},"outputs":[{"name":"stderr","text":"2025-05-02 17:44:15.554255: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746207855.750275      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746207855.806680      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"NMBIC scans: 140\nMBIC scans: 80\nNumber of samples in train and validation are 154 and 66.\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1746208033.323191      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Dimension of the MRI scan is: (128, 128, 64, 1)\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1746208040.053168      31 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"77/77 - 24s - 316ms/step - acc: 0.5195 - loss: 0.7545 - val_acc: 0.6364 - val_loss: 0.6581\nEpoch 2/100\n77/77 - 20s - 257ms/step - acc: 0.5714 - loss: 0.7323 - val_acc: 0.6364 - val_loss: 0.6907\nEpoch 3/100\n77/77 - 20s - 257ms/step - acc: 0.5844 - loss: 0.6903 - val_acc: 0.6364 - val_loss: 0.6583\nEpoch 4/100\n77/77 - 19s - 249ms/step - acc: 0.6104 - loss: 0.7198 - val_acc: 0.3636 - val_loss: 0.9983\nEpoch 5/100\n77/77 - 20s - 254ms/step - acc: 0.5584 - loss: 0.7036 - val_acc: 0.6364 - val_loss: 0.6782\nEpoch 6/100\n77/77 - 19s - 247ms/step - acc: 0.5844 - loss: 0.6846 - val_acc: 0.6364 - val_loss: 0.7025\nEpoch 7/100\n77/77 - 19s - 252ms/step - acc: 0.6104 - loss: 0.6865 - val_acc: 0.6364 - val_loss: 0.7736\nEpoch 8/100\n77/77 - 19s - 248ms/step - acc: 0.6234 - loss: 0.6758 - val_acc: 0.6364 - val_loss: 0.6688\nEpoch 9/100\n77/77 - 19s - 249ms/step - acc: 0.6429 - loss: 0.6559 - val_acc: 0.6364 - val_loss: 0.7816\nEpoch 10/100\n77/77 - 19s - 252ms/step - acc: 0.6104 - loss: 0.7005 - val_acc: 0.6515 - val_loss: 0.6950\nEpoch 11/100\n77/77 - 19s - 248ms/step - acc: 0.5909 - loss: 0.6781 - val_acc: 0.6364 - val_loss: 0.7318\nEpoch 12/100\n77/77 - 20s - 254ms/step - acc: 0.6039 - loss: 0.6890 - val_acc: 0.4545 - val_loss: 0.7726\nEpoch 13/100\n77/77 - 19s - 249ms/step - acc: 0.6299 - loss: 0.6782 - val_acc: 0.6364 - val_loss: 0.9429\nEpoch 14/100\n77/77 - 19s - 249ms/step - acc: 0.6039 - loss: 0.6782 - val_acc: 0.6818 - val_loss: 0.6827\nEpoch 15/100\n77/77 - 19s - 253ms/step - acc: 0.5974 - loss: 0.6748 - val_acc: 0.6364 - val_loss: 0.6694\nEpoch 16/100\n77/77 - 19s - 249ms/step - acc: 0.6234 - loss: 0.6622 - val_acc: 0.6364 - val_loss: 0.6991\nEpoch 17/100\n77/77 - 19s - 253ms/step - acc: 0.5844 - loss: 0.6624 - val_acc: 0.6364 - val_loss: 0.7521\nEpoch 18/100\n77/77 - 19s - 252ms/step - acc: 0.5779 - loss: 0.6942 - val_acc: 0.6364 - val_loss: 0.6594\nEpoch 19/100\n77/77 - 19s - 250ms/step - acc: 0.6364 - loss: 0.6584 - val_acc: 0.6364 - val_loss: 0.8726\nEpoch 20/100\n77/77 - 19s - 253ms/step - acc: 0.6169 - loss: 0.6924 - val_acc: 0.3636 - val_loss: 0.7625\nEpoch 21/100\n77/77 - 19s - 246ms/step - acc: 0.6169 - loss: 0.6632 - val_acc: 0.6364 - val_loss: 0.6814\nEpoch 22/100\n77/77 - 19s - 252ms/step - acc: 0.6364 - loss: 0.6641 - val_acc: 0.6364 - val_loss: 0.6652\nEpoch 23/100\n77/77 - 19s - 250ms/step - acc: 0.6494 - loss: 0.6416 - val_acc: 0.6364 - val_loss: 0.6970\nEpoch 24/100\n77/77 - 19s - 249ms/step - acc: 0.6169 - loss: 0.6685 - val_acc: 0.3636 - val_loss: 1.0996\nEpoch 25/100\n77/77 - 20s - 254ms/step - acc: 0.6234 - loss: 0.6623 - val_acc: 0.6515 - val_loss: 0.7220\nEpoch 26/100\n77/77 - 19s - 246ms/step - acc: 0.6494 - loss: 0.6539 - val_acc: 0.3636 - val_loss: 0.9550\nEpoch 27/100\n77/77 - 19s - 253ms/step - acc: 0.6494 - loss: 0.6595 - val_acc: 0.5909 - val_loss: 0.7445\nEpoch 28/100\n77/77 - 20s - 255ms/step - acc: 0.6558 - loss: 0.6537 - val_acc: 0.3636 - val_loss: 1.2467\nEpoch 29/100\n77/77 - 19s - 251ms/step - acc: 0.6234 - loss: 0.6728 - val_acc: 0.6364 - val_loss: 0.6747\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\nThis model is 66.68 percent confident that MRI scan is NMBIC\nThis model is 33.32 percent confident that MRI scan is MBIC\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Make predictions on a single MRI scan from a specified .nii file path\ndef predict_single_scan(model, nii_path):\n    \"\"\"Process and predict on a single .nii MRI scan.\"\"\"\n    # Process the scan\n    volume = process_scan(nii_path)\n    # Add batch and channel dimensions\n    volume = np.expand_dims(volume, axis=0)  # Shape: (1, 128, 128, 64)\n    volume = np.expand_dims(volume, axis=-1)  # Shape: (1, 128, 128, 64, 1)\n    # Load best weights\n    model.load_weights(\"3d_bladder_classification.keras\")\n    # Make prediction\n    prediction = model.predict(volume)[0]\n    scores = [1 - prediction[0], prediction[0]]\n    class_names = [\"NMBIC\", \"MBIC\"]\n    for score, name in zip(scores, class_names):\n        print(f\"This model is {100 * score:.2f} percent confident that MRI scan is {name}\")\n\n# Example usage: specify the path to your .nii file\nnii_file_path = \"/kaggle/input/testing/019 zhong guo ai.nii\"  # Replace with actual .nii file path\npredict_single_scan(model, nii_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:24:12.084069Z","iopub.execute_input":"2025-05-02T18:24:12.084784Z","iopub.status.idle":"2025-05-02T18:24:13.865037Z","shell.execute_reply.started":"2025-05-02T18:24:12.084753Z","shell.execute_reply":"2025-05-02T18:24:13.864332Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\nThis model is 65.68 percent confident that MRI scan is NMBIC\nThis model is 34.32 percent confident that MRI scan is MBIC\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nimport keras\nfrom keras import layers\nimport nibabel as nib\nfrom scipy import ndimage\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Define paths to your dataset\ndata_dir = \"/kaggle/input/all-zendo-dataset/DATA\"\nt2wi_dir = os.path.join(data_dir, \"T2WI\")\n\n\ncsv_path = \"/kaggle/input/all-zendo-dataset/DATA/all_centers_combined.csv\"\n\n# Load the CSV file\ndf = pd.read_csv(csv_path)\n# df = pd.read_excel(csv_path)\ndf['image_name'] = df['image_name'].str.replace('.nii.gz', '.nii')\n\n# Helper functions for preprocessing\ndef read_nifti_file(filepath):\n    \"\"\"Read and load volume\"\"\"\n    scan = nib.load(filepath)\n    scan = scan.get_fdata()\n    return scan\n\ndef normalize(volume):\n    \"\"\"Normalize the volume\"\"\"\n    min_hu = -1000\n    max_hu = 400\n    volume[volume < min_hu] = min_hu\n    volume[volume > max_hu] = max_hu\n    volume = (volume - min_hu) / (max_hu - min_hu)\n    volume = volume.astype(\"float32\")\n    return volume\n\ndef resize_volume(img):\n    \"\"\"Resize across z-axis\"\"\"\n    desired_depth = 64\n    desired_width = 128\n    desired_height = 128\n    current_depth = img.shape[-1]\n    current_width = img.shape[0]\n    current_height = img.shape[1]\n    depth = current_depth / desired_depth\n    width = current_width / desired_width\n    height = current_height / desired_height\n    depth_factor = 1 / depth\n    width_factor = 1 / width\n    height_factor = 1 / height\n    img = ndimage.rotate(img, 90, reshape=False)\n    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n    return img\n\ndef process_scan(path):\n    \"\"\"Read and resize volume\"\"\"\n    volume = read_nifti_file(path)\n    volume = normalize(volume)\n    volume = resize_volume(volume)\n    return volume\n\n# Load and pair T2WI images with labels\nnmbic_scan_paths = []\nmbic_scan_paths = []\nfor t2wi_subfolder in os.listdir(t2wi_dir):\n    subfolder_path = os.path.join(t2wi_dir, t2wi_subfolder)\n    if not os.path.isdir(subfolder_path):\n        continue\n    t2wi_files = [f for f in os.listdir(subfolder_path) if f.endswith(\".nii\")]\n    if t2wi_files:\n        t2wi_path = os.path.join(subfolder_path, t2wi_files[0])\n        t2wi_id = t2wi_subfolder.split(\".\")[0]\n        label_row = df[df['image_name'].str.replace('.nii', '') == t2wi_id]\n        if not label_row.empty:\n            label = label_row['label'].iloc[0]\n            if label == 0:\n                nmbic_scan_paths.append(t2wi_path)\n            else:\n                mbic_scan_paths.append(t2wi_path)\n        else:\n            print(f\"No label found for T2WI ID {t2wi_id}\")\n\nprint(f\"NMBIC scans: {len(nmbic_scan_paths)}\")\nprint(f\"MBIC scans: {len(mbic_scan_paths)}\")\n\n# Process scans\nnmbic_scans = np.array([process_scan(path) for path in nmbic_scan_paths])\nmbic_scans = np.array([process_scan(path) for path in mbic_scan_paths])\n\n# Assign labels (0 for NMBIC, 1 for MBIC)\nnmbic_labels = np.array([0 for _ in range(len(nmbic_scans))])\nmbic_labels = np.array([1 for _ in range(len(mbic_scans))])\n\n# Split data into training and validation (70-30)\nx_train = np.concatenate((mbic_scans[:int(0.7 * len(mbic_scans))], nmbic_scans[:int(0.7 * len(nmbic_scans))]), axis=0)\ny_train = np.concatenate((mbic_labels[:int(0.7 * len(mbic_labels))], nmbic_labels[:int(0.7 * len(nmbic_labels))]), axis=0)\nx_val = np.concatenate((mbic_scans[int(0.7 * len(mbic_scans)):], nmbic_scans[int(0.7 * len(nmbic_scans)):]), axis=0)\ny_val = np.concatenate((mbic_labels[int(0.7 * len(mbic_labels)):], nmbic_labels[int(0.7 * len(nmbic_labels)):]), axis=0)\n\nprint(f\"Number of samples in train and validation are {x_train.shape[0]} and {x_val.shape[0]}.\")\n\n# Data augmentation\ndef rotate(volume):\n    \"\"\"Rotate the volume by a few degrees\"\"\"\n    def scipy_rotate(volume):\n        angles = [-20, -10, -5, 5, 10, 20]\n        angle = np.random.choice(angles)\n        volume = ndimage.rotate(volume, angle, reshape=False)\n        volume[volume < 0] = 0\n        volume[volume > 1] = 1\n        return volume\n    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n    return augmented_volume\n\ndef train_preprocessing(volume, label):\n    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n    volume = rotate(volume)\n    volume = tf.expand_dims(volume, axis=3)\n    return volume, label\n\ndef validation_preprocessing(volume, label):\n    \"\"\"Process validation data by only adding a channel.\"\"\"\n    volume = tf.expand_dims(volume, axis=3)\n    return volume, label\n\n# Define data loaders\ntrain_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\nvalidation_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n\nbatch_size = 2\ntrain_dataset = (\n    train_loader.shuffle(len(x_train))\n    .map(train_preprocessing, num_parallel_calls=tf.data.AUTOTUNE)\n    .batch(batch_size)\n    .prefetch(2)\n)\nvalidation_dataset = (\n    validation_loader.shuffle(len(x_val))\n    .map(validation_preprocessing, num_parallel_calls=tf.data.AUTOTUNE)\n    .batch(batch_size)\n    .prefetch(2)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:27:32.033220Z","iopub.execute_input":"2025-05-02T20:27:32.033828Z","iopub.status.idle":"2025-05-02T20:30:50.988022Z","shell.execute_reply.started":"2025-05-02T20:27:32.033789Z","shell.execute_reply":"2025-05-02T20:30:50.987094Z"}},"outputs":[{"name":"stdout","text":"NMBIC scans: 140\nMBIC scans: 80\nNumber of samples in train and validation are 154 and 66.\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1746217848.624564      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# # Visualize an augmented CT scan\n# data = train_dataset.take(1)\n# images, labels = list(data)[0]\n# images = images.numpy()\n# image = images[0]\n# print(\"Dimension of the MRI scan is:\", image.shape)\n# plt.figure()\n# plt.imshow(np.squeeze(image[:, :, 30]), cmap=\"gray\")\n# plt.close()\n\n# # Visualize montage of slices\n# def plot_slices(num_rows, num_columns, width, height, data):\n#     \"\"\"Plot a montage of MRI slices\"\"\"\n#     data = np.rot90(np.array(data))\n#     data = np.transpose(data)\n#     data = np.reshape(data, (num_rows, num_columns, width, height))\n#     rows_data, columns_data = data.shape[0], data.shape[1]\n#     heights = [slc[0].shape[0] for slc in data]\n#     widths = [slc.shape[1] for slc in data[0]]\n#     fig_width = 12.0\n#     fig_height = fig_width * sum(heights) / sum(widths)\n#     f, axarr = plt.subplots(\n#         rows_data,\n#         columns_data,\n#         figsize=(fig_width, fig_height),\n#         gridspec_kw={\"height_ratios\": heights},\n#     )\n#     for i in range(rows_data):\n#         for j in range(columns_data):\n#             axarr[i, j].imshow(data[i][j], cmap=\"gray\")\n#             axarr[i, j].axis(\"off\")\n#     plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n#     plt.savefig('mri_slices_montage.png')\n#     plt.close()\n\n# plot_slices(4, 10, 128, 128, image[:, :, :40])\n\n# # Define 3D CNN model\n# def get_model(width=128, height=128, depth=64):\n#     \"\"\"Build a 3D convolutional neural network model.\"\"\"\n#     inputs = keras.Input((width, height, depth, 1))\n#     x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n#     x = layers.MaxPool3D(pool_size=2)(x)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n#     x = layers.MaxPool3D(pool_size=2)(x)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n#     x = layers.MaxPool3D(pool_size=2)(x)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n#     x = layers.MaxPool3D(pool_size=2)(x)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.GlobalAveragePooling3D()(x)\n#     x = layers.Dense(units=512, activation=\"relu\")(x)\n#     x = layers.Dropout(0.3)(x)\n#     outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n#     model = keras.Model(inputs, outputs, name=\"3dcnn\")\n#     return model\n\n# # Build and compile model\n# model = get_model(width=128, height=128, depth=64)\n# initial_learning_rate = 0.0001\n# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n# )\n# model.compile(\n#     loss=\"binary_crossentropy\",\n#     optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n#     metrics=[\"acc\"],\n#     run_eagerly=True,\n# )\n\n# # Define callbacks\n# checkpoint_cb = keras.callbacks.ModelCheckpoint(\n#     \"3d_bladder_classification.keras\", save_best_only=True\n# )\n# early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n\n# # Train the model\n# epochs = 100\n# history = model.fit(\n#     train_dataset,\n#     validation_data=validation_dataset,\n#     epochs=epochs,\n#     shuffle=True,\n#     verbose=2,\n#     callbacks=[checkpoint_cb, early_stopping_cb],\n# )\n# # Visualize an augmented CT scan\n# data = train_dataset.take(1)\n# images, labels = list(data)[0]\n# images = images.numpy()\n# image = images[0]\n# print(\"Dimension of the MRI scan is:\", image.shape)\n# plt.figure()\n# plt.imshow(np.squeeze(image[:, :, 30]), cmap=\"gray\")\n# plt.savefig('sample_mri_scan.png')\n# plt.close()\n\n# # Visualize montage of slices\n# def plot_slices(num_rows, num_columns, width, height, data):\n#     \"\"\"Plot a montage of MRI slices\"\"\"\n#     data = np.rot90(np.array(data))\n#     data = np.transpose(data)\n#     data = np.reshape(data, (num_rows, num_columns, width, height))\n#     rows_data, columns_data = data.shape[0], data.shape[1]\n#     heights = [slc[0].shape[0] for slc in data]\n#     widths = [slc.shape[1] for slc in data[0]]\n#     fig_width = 12.0\n#     fig_height = fig_width * sum(heights) / sum(widths)\n#     f, axarr = plt.subplots(\n#         rows_data,\n#         columns_data,\n#         figsize=(fig_width, fig_height),\n#         gridspec_kw={\"height_ratios\": heights},\n#     )\n#     for i in range(rows_data):\n#         for j in range(columns_data):\n#             axarr[i, j].imshow(data[i][j], cmap=\"gray\")\n#             axarr[i, j].axis(\"off\")\n#     plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n#     plt.savefig('mri_slices_montage.png')\n#     plt.close()\n\n# plot_slices(4, 10, 128, 128, image[:, :, :40])\n\n# # Define 3D CNN model\n# def get_model(width=128, height=128, depth=64):\n#     \"\"\"Build a 3D convolutional neural network model.\"\"\"\n#     inputs = keras.Input((width, height, depth, 1))\n#     x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n#     x = layers.MaxPool3D(pool_size=2)(x)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n#     x = layers.MaxPool3D(pool_size=2)(x)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n#     x = layers.MaxPool3D(pool_size=2)(x)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n#     x = layers.MaxPool3D(pool_size=2)(x)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.GlobalAveragePooling3D()(x)\n#     x = layers.Dense(units=512, activation=\"relu\")(x)\n#     x = layers.Dropout(0.3)(x)\n#     outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n#     model = keras.Model(inputs, outputs, name=\"3dcnn\")\n#     return model\n\n# # Build and compile model\n# model = get_model(width=128, height=128, depth=64)\n# initial_learning_rate = 0.0001\n# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n# )\n# model.compile(\n#     loss=\"binary_crossentropy\",\n#     optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n#     metrics=[\"acc\"],\n#     run_eagerly=True,\n# )\n\n# # Define callbacks\n# checkpoint_cb = keras.callbacks.ModelCheckpoint(\n#     \"3d_bladder_classification.keras\", save_best_only=True\n# )\n# early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n\n# # Assume model has already been trained and history is available\n# # If history is not available, you need to load it from a saved file or ensure training occurred\n# # For demonstration, we'll assume history exists from previous training\n# # If you have a saved history, you can load it, e.g., with pickle:\n# # import pickle\n# # with open('training_history.pkl', 'rb') as f:\n# #     history = pickle.load(f)\n\n# # Plot model performance (accuracy and loss)\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# import numpy as np\n# from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n\n# fig, ax = plt.subplots(1, 2, figsize=(20, 3))\n# ax = ax.ravel()\n# for i, metric in enumerate([\"acc\", \"loss\"]):\n#     ax[i].plot(history.history[metric])\n#     ax[i].plot(history.history[\"val_\" + metric])\n#     ax[i].set_title(f\"Model {metric}\")\n#     ax[i].set_xlabel(\"epochs\")\n#     ax[i].set_ylabel(metric)\n#     ax[i].legend([\"train\", \"val\"])\n# plt.savefig('model_performance_acc_loss.png')\n# plt.close()\n\n# # Evaluate model on train and validation sets for final metrics\n# model.load_weights(\"3d_bladder_classification.keras\")\n\n# # Get predictions and true labels for training set\n# train_true = []\n# train_pred = []\n# for batch in train_dataset:\n#     images, labels = batch\n#     predictions = model.predict(images, verbose=0)\n#     train_true.extend(labels.numpy().flatten())\n#     train_pred.extend((predictions > 0.5).astype(int).flatten())\n\n# train_true = np.array(train_true)\n# train_pred = np.array(train_pred)\n\n# # Get predictions and true labels for validation set\n# val_true = []\n# val_pred = []\n# for batch in validation_dataset:\n#     images, labels = batch\n#     predictions = model.predict(images, verbose=0)\n#     val_true.extend(labels.numpy().flatten())\n#     val_pred.extend((predictions > 0.5).astype(int).flatten())\n\n# val_true = np.array(val_true)\n# val_pred = np.array(val_pred)\n\n# # Compute final metrics\n# train_precision = precision_score(train_true, train_pred, zero_division=0)\n# train_recall = recall_score(train_true, train_pred, zero_division=0)\n# train_accuracy = accuracy_score(train_true, train_pred)\n\n# val_precision = precision_score(val_true, val_pred, zero_division=0)\n# val_recall = recall_score(val_true, val_pred, zero_division=0)\n# val_accuracy = accuracy_score(val_true, val_pred)\n\n# print(\"\\nFinal Training Set Metrics:\")\n# print(f\"Precision: {train_precision:.4f}\")\n# print(f\"Recall: {train_recall:.4f}\")\n# print(f\"Accuracy: {train_accuracy:.4f}\")\n\n# print(\"\\nFinal Validation Set Metrics:\")\n# print(f\"Precision: {val_precision:.4f}\")\n# print(f\"Recall: {val_recall:.4f}\")\n# print(f\"Accuracy: {val_accuracy:.4f}\")\n\n# # Plot precision, recall, and accuracy\n# fig, ax = plt.subplots(1, 3, figsize=(20, 3))\n# ax = ax.ravel()\n# metrics = [\"precision\", \"recall\", \"accuracy\"]\n# train_metrics = [train_precision, train_recall, train_accuracy]\n# val_metrics = [val_precision, val_recall, val_accuracy]\n\n# for i, metric in enumerate(metrics):\n#     ax[i].bar(['Train', 'Validation'], [train_metrics[i], val_metrics[i]], color=['blue', 'orange'])\n#     ax[i].set_title(f\"Model {metric.capitalize()}\")\n#     ax[i].set_ylabel(metric.capitalize())\n#     ax[i].set_ylim(0, 1)\n# plt.savefig('model_performance_metrics.png')\n# plt.close()\n\n# # Plot confusion matrix for validation set\n# cm = confusion_matrix(val_true, val_pred)\n# plt.figure(figsize=(6, 6))\n# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['NMBIC', 'MBIC'], yticklabels=['NMBIC', 'MBIC'])\n# plt.xlabel('Predicted')\n# plt.ylabel('True')\n# plt.title('Confusion Matrix')\n# plt.savefig('confusion_matrix.png')\n# plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T19:05:36.774562Z","iopub.execute_input":"2025-05-02T19:05:36.775265Z","iopub.status.idle":"2025-05-02T19:08:50.209410Z","shell.execute_reply.started":"2025-05-02T19:05:36.775233Z","shell.execute_reply":"2025-05-02T19:08:50.208795Z"}},"outputs":[{"name":"stdout","text":"Dimension of the MRI scan is: (128, 128, 64, 1)\nEpoch 1/100\n41/41 - 11s - 259ms/step - acc: 0.6951 - loss: 0.6555 - val_acc: 0.2432 - val_loss: 0.7421\nEpoch 2/100\n41/41 - 11s - 258ms/step - acc: 0.7561 - loss: 0.6411 - val_acc: 0.7568 - val_loss: 0.6638\nEpoch 3/100\n41/41 - 11s - 265ms/step - acc: 0.7439 - loss: 0.6324 - val_acc: 0.7568 - val_loss: 0.5674\nEpoch 4/100\n41/41 - 11s - 257ms/step - acc: 0.7683 - loss: 0.6049 - val_acc: 0.7568 - val_loss: 0.6431\nEpoch 5/100\n41/41 - 11s - 259ms/step - acc: 0.7927 - loss: 0.5751 - val_acc: 0.7568 - val_loss: 0.9135\nEpoch 6/100\n41/41 - 11s - 262ms/step - acc: 0.7317 - loss: 0.6185 - val_acc: 0.7568 - val_loss: 0.8002\nEpoch 7/100\n41/41 - 11s - 256ms/step - acc: 0.7805 - loss: 0.5622 - val_acc: 0.7568 - val_loss: 0.9744\nEpoch 8/100\n41/41 - 10s - 255ms/step - acc: 0.7805 - loss: 0.5927 - val_acc: 0.7568 - val_loss: 1.1791\nEpoch 9/100\n41/41 - 11s - 261ms/step - acc: 0.7927 - loss: 0.5638 - val_acc: 0.7568 - val_loss: 1.1255\nEpoch 10/100\n41/41 - 10s - 256ms/step - acc: 0.8049 - loss: 0.5325 - val_acc: 0.7568 - val_loss: 0.6957\nEpoch 11/100\n41/41 - 10s - 254ms/step - acc: 0.7683 - loss: 0.5320 - val_acc: 0.7568 - val_loss: 0.6359\nEpoch 12/100\n41/41 - 11s - 261ms/step - acc: 0.7927 - loss: 0.5703 - val_acc: 0.7568 - val_loss: 0.6584\nEpoch 13/100\n41/41 - 11s - 259ms/step - acc: 0.7561 - loss: 0.5628 - val_acc: 0.7568 - val_loss: 0.6917\nEpoch 14/100\n41/41 - 10s - 253ms/step - acc: 0.7805 - loss: 0.5742 - val_acc: 0.7568 - val_loss: 0.6296\nEpoch 15/100\n41/41 - 11s - 257ms/step - acc: 0.7805 - loss: 0.5395 - val_acc: 0.7568 - val_loss: 0.7137\nEpoch 16/100\n41/41 - 11s - 257ms/step - acc: 0.7439 - loss: 0.5658 - val_acc: 0.7568 - val_loss: 1.3662\nEpoch 17/100\n41/41 - 10s - 256ms/step - acc: 0.7805 - loss: 0.5869 - val_acc: 0.7568 - val_loss: 0.7500\nDimension of the MRI scan is: (128, 128, 64, 1)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 1 variables whereas the saved optimizer has 41 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"},{"name":"stdout","text":"\nFinal Training Set Metrics:\nPrecision: 0.0000\nRecall: 0.0000\nAccuracy: 0.7805\n\nFinal Validation Set Metrics:\nPrecision: 0.0000\nRecall: 0.0000\nAccuracy: 0.7568\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"\"\"\"\nEnd‑to‑end script for:\n\n1. Visualising one augmented MRI scan + a montage of slices (saved as PNGs).\n2. Building, training and saving a 3‑D CNN on binary bladder‑cancer labels.\n3. Plotting **training vs validation** curves for accuracy, precision, recall,\n   AUC and loss.\n4. Computing final set‑level metrics (precision, recall, accuracy, AUC),\n   plotting them side‑by‑side, and drawing a confusion‑matrix heat‑map.\n\n📌  Requirements\n---------------------------------------------------------------------------\n• TensorFlow 2.x  (incl. Keras API)          pip install tensorflow\n• scikit‑learn                                pip install scikit-learn\n• seaborn                                     pip install seaborn\n• matplotlib                                  pip install matplotlib\n• numpy                                       pip install numpy\n\nThe code assumes you already created two tf.data.Dataset objects:\n    train_dataset       – yielding (volume, label) with shapes\n                           (128,128,64,1) float32  and  () int32/float32\n    validation_dataset  – same as above\n\nLabels must be binary (0 = NMIBC, 1 = MIBC).\n\nIf your dataset pipeline is named differently, just replace the variables\n`train_dataset` and `validation_dataset` before running.\n---------------------------------------------------------------------------\n\"\"\"\n\n# --------------------------------------------------------------------------\n# Imports\n# --------------------------------------------------------------------------\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    precision_score,\n    recall_score,\n    accuracy_score,\n    roc_auc_score,\n    confusion_matrix,\n)\n\n# --------------------------------------------------------------------------\n# 1.  Visualise one training scan + montage of 40 slices\n# --------------------------------------------------------------------------\ndef plot_slices(num_rows, num_columns, width, height, data, out_path):\n    \"\"\"Create and save a montage of 2‑D slices from a 3‑D MRI volume.\"\"\"\n    data = np.rot90(np.array(data))            # rotate so cranial‑caudal reads L‑>R\n    data = np.transpose(data)                  # swap axes for nicer montage\n    data = np.reshape(data, (num_rows, num_columns, width, height))\n\n    rows_data, columns_data = data.shape[:2]\n    heights = [slc[0].shape[0] for slc in data]\n    widths  = [slc.shape[1]    for slc in data[0]]\n\n    fig_width  = 12.0\n    fig_height = fig_width * sum(heights) / sum(widths)\n\n    f, axarr = plt.subplots(\n        rows_data,\n        columns_data,\n        figsize=(fig_width, fig_height),\n        gridspec_kw={\"height_ratios\": heights},\n    )\n\n    for i in range(rows_data):\n        for j in range(columns_data):\n            axarr[i, j].imshow(data[i][j], cmap=\"gray\")\n            axarr[i, j].axis(\"off\")\n\n    plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n    plt.close()\n\n\n# grab one mini‑batch and visualise\nsample_batch = next(iter(train_dataset.take(1)))\nvolumes, labels = sample_batch\nvolumes = volumes.numpy()\nsample_vol = volumes[0]\n\n# single mid‑slice preview\nplt.figure()\nplt.imshow(np.squeeze(sample_vol[:, :, sample_vol.shape[2] // 2]), cmap=\"gray\")\nplt.axis(\"off\")\nplt.title(\"Sample mid‑slice\")\nplt.savefig(\"sample_mri_slice.png\", dpi=200, bbox_inches=\"tight\")\nplt.close()\n\n# montage of first 40 slices (4 × 10 grid)\nplot_slices(\n    num_rows=4,\n    num_columns=10,\n    width=128,\n    height=128,\n    data=sample_vol[:, :, :40],\n    out_path=\"mri_slices_montage.png\",\n)\n\n# --------------------------------------------------------------------------\n# 2.  Define, compile and train the 3‑D CNN\n# --------------------------------------------------------------------------\ndef build_3d_cnn(width=128, height=128, depth=64):\n    inputs = keras.Input(shape=(width, height, depth, 1))\n\n    x = layers.Conv3D(64, 3, activation=\"relu\")(inputs)\n    x = layers.MaxPool3D(2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv3D(64, 3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv3D(128, 3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv3D(256, 3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.GlobalAveragePooling3D()(x)\n    x = layers.Dense(512, activation=\"relu\")(x)\n    x = layers.Dropout(0.3)(x)\n\n    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n    return keras.Model(inputs, outputs, name=\"3d_cnn\")\n\n\nmodel = build_3d_cnn()\n\ninitial_lr   = 1e-4\nlr_schedule  = keras.optimizers.schedules.ExponentialDecay(\n    initial_lr, decay_steps=100_000, decay_rate=0.96, staircase=True\n)\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n    loss=\"binary_crossentropy\",\n    metrics=[\n        keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n        keras.metrics.Precision(name=\"precision\"),\n        keras.metrics.Recall(name=\"recall\"),\n        keras.metrics.AUC(name=\"auc\"),\n    ],\n    run_eagerly=False,\n)\n\ncheckpoint_cb   = keras.callbacks.ModelCheckpoint(\n    \"3d_bladder_classification.keras\", save_best_only=True\n)\nearlystop_cb    = keras.callbacks.EarlyStopping(\n    monitor=\"val_auc\", patience=15, restore_best_weights=True\n)\n\nEPOCHS = 100\nhistory = model.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    epochs=EPOCHS,\n    shuffle=True,\n    callbacks=[checkpoint_cb, earlystop_cb],\n    verbose=2,\n)\n\n# --------------------------------------------------------------------------\n# 3.  Plot training vs validation curves\n# --------------------------------------------------------------------------\nplt.figure(figsize=(18, 10))\n\nmetrics_to_plot = [\"accuracy\", \"precision\", \"recall\", \"auc\", \"loss\"]\nfor idx, m in enumerate(metrics_to_plot, 1):\n    plt.subplot(2, 3, idx)\n    plt.plot(history.history[m], label=f\"train_{m}\")\n    plt.plot(history.history[f\"val_{m}\"], label=f\"val_{m}\")\n    plt.title(m.capitalize())\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(m.capitalize())\n    plt.legend()\n    plt.grid(True)\n\nplt.tight_layout()\nplt.savefig(\"training_validation_curves.png\", dpi=200)\nplt.close()\n\n# --------------------------------------------------------------------------\n# 4.  Evaluate on train & validation sets for final metrics\n# --------------------------------------------------------------------------\ndef evaluate_dataset(ds):\n    y_true, y_pred_bin, y_prob = [], [], []\n    for vol_batch, lbl_batch in ds:\n        prob = model.predict(vol_batch, verbose=0).flatten()\n        y_prob.extend(prob)\n        y_pred_bin.extend((prob > 0.5).astype(int))\n        y_true.extend(lbl_batch.numpy().astype(int))\n    y_true      = np.array(y_true)\n    y_pred_bin  = np.array(y_pred_bin)\n    y_prob      = np.array(y_prob)\n    prec  = precision_score(y_true, y_pred_bin, zero_division=0)\n    rec   = recall_score(y_true, y_pred_bin, zero_division=0)\n    acc   = accuracy_score(y_true, y_pred_bin)\n    auc_  = roc_auc_score(y_true, y_prob)\n    return prec, rec, acc, auc_\n\n\ntrain_prec, train_rec, train_acc, train_auc = evaluate_dataset(train_dataset)\nval_prec,   val_rec,   val_acc,   val_auc   = evaluate_dataset(validation_dataset)\n\nprint(\"\\n=== Final Training‑set metrics ===\")\nprint(f\"Precision : {train_prec:.4f}\")\nprint(f\"Recall    : {train_rec:.4f}\")\nprint(f\"Accuracy  : {train_acc:.4f}\")\nprint(f\"AUC       : {train_auc:.4f}\")\n\nprint(\"\\n=== Final Validation‑set metrics ===\")\nprint(f\"Precision : {val_prec:.4f}\")\nprint(f\"Recall    : {val_rec:.4f}\")\nprint(f\"Accuracy  : {val_acc:.4f}\")\nprint(f\"AUC       : {val_auc:.4f}\")\n\n# bar‑chart comparison\nbar_metrics      = [\"Precision\", \"Recall\", \"Accuracy\", \"AUC\"]\ntrain_bar_values = [train_prec, train_rec, train_acc, train_auc]\nval_bar_values   = [val_prec, val_rec, val_acc, val_auc]\n\nplt.figure(figsize=(10, 4))\nsns.barplot(x=bar_metrics, y=train_bar_values, color=\"steelblue\", label=\"Train\")\nsns.barplot(x=bar_metrics, y=val_bar_values,   color=\"orange\",    label=\"Validation\")\nplt.ylim(0, 1)\nplt.title(\"Final metrics comparison\")\nplt.legend()\nplt.savefig(\"final_metrics_barplot.png\", dpi=200, bbox_inches=\"tight\")\nplt.close()\n\n# --------------------------------------------------------------------------\n# 5.  Confusion matrix on the validation set\n# --------------------------------------------------------------------------\nval_true_bin, val_pred_bin = [], []\nfor vol_batch, lbl_batch in validation_dataset:\n    prob_batch = model.predict(vol_batch, verbose=0).flatten()\n    val_true_bin.extend(lbl_batch.numpy().astype(int))\n    val_pred_bin.extend((prob_batch > 0.5).astype(int))\n\ncm = confusion_matrix(val_true_bin, val_pred_bin)\nplt.figure(figsize=(6, 5))\nsns.heatmap(\n    cm,\n    annot=True,\n    fmt=\"d\",\n    cmap=\"Blues\",\n    xticklabels=[\"NMIBC\", \"MIBC\"],\n    yticklabels=[\"NMIBC\", \"MIBC\"],\n)\nplt.xlabel(\"Predicted label\")\nplt.ylabel(\"True label\")\nplt.title(\"Confusion matrix (validation)\")\nplt.savefig(\"confusion_matrix.png\", dpi=200, bbox_inches=\"tight\")\nplt.close()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:51:31.593317Z","iopub.execute_input":"2025-05-02T20:51:31.593964Z","iopub.status.idle":"2025-05-02T20:51:37.018175Z","shell.execute_reply.started":"2025-05-02T20:51:31.593937Z","shell.execute_reply":"2025-05-02T20:51:37.017124Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/4247343128.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: as_list() is not defined on an unknown TensorShape."],"ename":"ValueError","evalue":"as_list() is not defined on an unknown TensorShape.","output_type":"error"}],"execution_count":4}]}