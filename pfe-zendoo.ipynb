{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11526622,"sourceType":"datasetVersion","datasetId":7229200},{"sourceId":11593193,"sourceType":"datasetVersion","datasetId":7269824}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Segmentation and classification\n","metadata":{}},{"cell_type":"markdown","source":"## Visualization ","metadata":{}},{"cell_type":"code","source":"import nibabel as nib\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom pathlib import Path\n\n# Define paths to your dataset\ndata_dir = \"/kaggle/input/zendo-fed-bca/FedBCa/Center1/\"  # Adjust this path to your dataset\nt2wi_dir = os.path.join(data_dir, \"T2WI\")\nannotation_dir = os.path.join(data_dir, \"Annotation\")\n\n# Function to load a NIfTI file\ndef load_nifti(file_path):\n    nifti = nib.load(file_path)\n    return nifti.get_fdata()\n\n# Get all T2WI subfolders and pair with annotations\nimage_mask_pairs = []\nannotation_files = [f for f in os.listdir(annotation_dir) if f.endswith(\".nii\")]\nfor t2wi_subfolder in os.listdir(t2wi_dir):\n    subfolder_path = os.path.join(t2wi_dir, t2wi_subfolder)\n    if not os.path.isdir(subfolder_path):\n        continue\n    t2wi_files = [f for f in os.listdir(subfolder_path) if f.endswith(\".nii\")]\n    for t2wi_file in t2wi_files:\n        t2wi_path = os.path.join(subfolder_path, t2wi_file)\n        t2wi_id = t2wi_subfolder.split(\".\")[0]  # e.g., \"001\"\n        matching_annotation = None\n        for ann_file in annotation_files:\n            if t2wi_id in ann_file:  # Adjust pairing logic as needed\n                matching_annotation = ann_file\n                break\n        if matching_annotation:\n            annotation_path = os.path.join(annotation_dir, matching_annotation)\n            image_mask_pairs.append((t2wi_path, annotation_path))\n        else:\n            print(f\"No matching annotation found for T2WI subfolder {t2wi_subfolder}\")\n\n# Load images and masks, and find representative slices\nimages, masks, identifiers, slice_indices = [], [], [], []\nfor t2wi_path, annotation_path in image_mask_pairs:\n    try:\n        img = load_nifti(t2wi_path)\n        mask = load_nifti(annotation_path)\n        img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n        mask = (mask > 0).astype(np.uint8)\n        tumor_slices = [i for i in range(mask.shape[2]) if mask[:, :, i].sum() > 0]\n        slice_idx = tumor_slices[len(tumor_slices) // 2] if tumor_slices else img.shape[2] // 2\n        images.append(img)\n        masks.append(mask)\n        identifiers.append(os.path.basename(t2wi_path))\n        slice_indices.append(slice_idx)\n    except Exception as e:\n        print(f\"Error loading {t2wi_path} or {annotation_path}: {e}\")\n\n# Visualize all images in a single grid\nnum_pairs = len(images)\ncols = 2  # Two columns per pair (T2WI and T2WI+mask)\nrows = num_pairs  # One row per image-mask pair\nplt.figure(figsize=(12, 5 * rows))\nfor i in range(num_pairs):\n    # Plot T2WI slice\n    plt.subplot(rows, cols, 2 * i + 1)\n    plt.imshow(images[i][:, :, slice_indices[i]], cmap=\"gray\")\n    plt.title(f\"T2WI {identifiers[i]} - Slice {slice_indices[i]}\")\n    plt.axis(\"off\")\n    \n    # Plot T2WI slice with mask overlaid\n    plt.subplot(rows, cols, 2 * i + 2)\n    plt.imshow(images[i][:, :, slice_indices[i]], cmap=\"gray\")\n    plt.imshow(masks[i][:, :, slice_indices[i]], cmap=\"jet\", alpha=0.5)\n    plt.title(f\"T2WI {identifiers[i]} - Slice {slice_indices[i]} with Mask\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T22:28:53.639489Z","iopub.execute_input":"2025-04-29T22:28:53.639676Z","iopub.status.idle":"2025-04-29T22:29:57.921264Z","shell.execute_reply.started":"2025-04-29T22:28:53.639657Z","shell.execute_reply":"2025-04-29T22:29:57.919986Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Segmenation ","metadata":{}},{"cell_type":"code","source":"# !pip install segmentation-models-pytorch\n\nimport nibabel as nib\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nfrom torchvision import transforms\nimport os\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# Define paths to your dataset\ndata_dir = \"/kaggle/input/all-zendo-dataset/DATA/\"  # Matches your dataset path\nt2wi_dir = os.path.join(data_dir, \"T2WI\")\nannotation_dir = os.path.join(data_dir, \"Annotation\")\n\n# Function to load a NIfTI file\ndef load_nifti(file_path):\n    nifti = nib.load(file_path)\n    return nifti.get_fdata()\n\n# Pair T2WI images with their annotations\nimage_mask_pairs = []\n# First, collect all T2WI files\nt2wi_files_dict = {}\nfor t2wi_subfolder in os.listdir(t2wi_dir):\n    subfolder_path = os.path.join(t2wi_dir, t2wi_subfolder)\n    if not os.path.isdir(subfolder_path):\n        continue\n    t2wi_files = [f for f in os.listdir(subfolder_path) if f.endswith(\".nii\")]\n    if t2wi_files:  # Ensure there's at least one .nii file\n        t2wi_path = os.path.join(subfolder_path, t2wi_files[0])  # Take the first .nii file\n        t2wi_id = t2wi_subfolder.split(\".\")[0]  # e.g., \"001\"\n        t2wi_files_dict[t2wi_id] = t2wi_path\n\n# Now, collect annotation files (handle both direct .nii files and subfolders)\nfor item in os.listdir(annotation_dir):\n    item_path = os.path.join(annotation_dir, item)\n    if os.path.isdir(item_path):  # Case 1: Subfolder like \"001.nii\"\n        ann_files = [f for f in os.listdir(item_path) if f.endswith(\".nii\")]\n        if ann_files:\n            ann_path = os.path.join(item_path, ann_files[0])\n            ann_id = item.split(\".\")[0]  # e.g., \"001\"\n            if ann_id in t2wi_files_dict:  # Match with T2WI\n                image_mask_pairs.append((t2wi_files_dict[ann_id], ann_path))\n    elif item.endswith(\".nii\"):  # Case 2: Direct .nii file\n        ann_id = item.split(\".\")[0]  # e.g., \"062YUANHEQ\" -> need to extract the ID part\n        # Adjust this based on your naming convention\n        # Assuming the ID is the first 3 characters (e.g., \"062\" for \"062YUANHEQ\")\n        possible_id = ann_id[:3] if len(ann_id) >= 3 else ann_id\n        if possible_id in t2wi_files_dict:\n            ann_path = item_path\n            image_mask_pairs.append((t2wi_files_dict[possible_id], ann_path))\n\n# Print the pairs to verify\nprint(\"Paired T2WI-Annotation files:\", len(image_mask_pairs))\nfor t2wi_path, ann_path in image_mask_pairs[:5]:  # Show first 5 pairs\n    print(f\"T2WI: {t2wi_path}, Annotation: {ann_path}\")\n\n# Define resize transform\nresize = transforms.Resize((256, 256))\n\n# Custom Dataset class\nclass BladderTumorDataset(Dataset):\n    def __init__(self, image_mask_pairs):\n        self.image_mask_pairs = image_mask_pairs\n        self.slices = []\n        for idx, (img_path, mask_path) in enumerate(self.image_mask_pairs):\n            try:\n                img = load_nifti(img_path)\n                mask = load_nifti(mask_path)\n                img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n                mask = (mask > 0).astype(np.uint8)\n                for z in range(mask.shape[2]):\n                    if mask[:, :, z].sum() > 0:\n                        self.slices.append((idx, z, img, mask))\n            except Exception as e:\n                print(f\"Error loading pair {img_path} and {mask_path}: {e}\")\n\n    def __len__(self):\n        return len(self.slices)\n\n    def __getitem__(self, idx):\n        img_idx, z, img, mask = self.slices[idx]\n        img_slice = img[:, :, z]\n        mask_slice = mask[:, :, z]\n        img_slice = torch.FloatTensor(img_slice).unsqueeze(0)\n        mask_slice = torch.LongTensor(mask_slice)\n        img_slice = resize(img_slice)\n        mask_slice = resize(mask_slice.unsqueeze(0)).squeeze(0)\n        return img_slice, mask_slice\n\n    def get_pair_index(self, idx):\n        return self.slices[idx][0]\n\n# Create dataset\ndataset = BladderTumorDataset(image_mask_pairs)\n\n# Split at the pair level\npair_indices = list(range(len(image_mask_pairs)))\ntrain_pair_indices, val_pair_indices = train_test_split(pair_indices, test_size=0.2, random_state=42)\ntrain_indices, val_indices = [], []\nfor slice_idx in range(len(dataset)):\n    pair_idx = dataset.get_pair_index(slice_idx)\n    if pair_idx in train_pair_indices:\n        train_indices.append(slice_idx)\n    elif pair_idx in val_pair_indices:\n        val_indices.append(slice_idx)\n\n# Create data loaders\ntrain_sampler = SubsetRandomSampler(train_indices)\nval_sampler = SubsetRandomSampler(val_indices)\ntrain_loader = DataLoader(dataset, batch_size=8, sampler=train_sampler)\nval_loader = DataLoader(dataset, batch_size=8, sampler=val_sampler)\n\n# Model setup and training\n!pip install segmentation-models-pytorch\nimport segmentation_models_pytorch as smp\nimport torch.optim as optim\n\nmodel = smp.Unet(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    in_channels=1,\n    classes=2,\n)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ncriterion = smp.losses.DiceLoss(mode=\"multiclass\")\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ndef dice_score(pred, target, smooth=1):\n    pred = torch.argmax(pred, dim=1)\n    intersection = (pred * target).sum()\n    return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n\n# Training loop with validation\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    for images_batch, masks_batch in train_loader:\n        images_batch, masks_batch = images_batch.to(device), masks_batch.to(device)\n        outputs = model(images_batch)\n        loss = criterion(outputs, masks_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    train_loss /= len(train_loader)\n    \n    model.eval()\n    val_loss = 0.0\n    val_dice = 0.0\n    with torch.no_grad():\n        for images_batch, masks_batch in val_loader:\n            images_batch, masks_batch = images_batch.to(device), masks_batch.to(device)\n            outputs = model(images_batch)\n            loss = criterion(outputs, masks_batch)\n            val_loss += loss.item()\n            dice = dice_score(outputs, masks_batch)\n            val_dice += dice.item()\n    val_loss /= len(val_loader)\n    val_dice /= len(val_loader)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}\")\n\n# Visualize predictions on validation set\nmodel.eval()\nval_iter = iter(val_loader)\nimages, masks = next(val_iter)  # Get one batch from validation set\nimages, masks = images.to(device), masks.to(device)\n\nwith torch.no_grad():\n    preds = model(images)\n    preds = torch.argmax(preds, dim=1)  # Convert logits to class predictions\n\n# Move to CPU for visualization\nimages, masks, preds = images.cpu(), masks.cpu(), preds.cpu()\n\n# Visualize the first 18 samples in the batch (or fewer if batch is smaller)\nnum_samples = min(18, len(images))\nplt.figure(figsize=(15, 4 * num_samples))\nfor i in range(num_samples):\n    # T2WI image\n    plt.subplot(num_samples, 3, 3 * i + 1)\n    plt.imshow(images[i, 0], cmap=\"gray\")\n    plt.title(\"T2WI Image\")\n    plt.axis(\"off\")\n    \n    # Ground truth mask\n    plt.subplot(num_samples, 3, 3 * i + 2)\n    plt.imshow(images[i, 0], cmap=\"gray\")\n    plt.imshow(masks[i], cmap=\"jet\", alpha=0.5)\n    plt.title(\"Ground Truth Mask\")\n    plt.axis(\"off\")\n    \n    # Predicted mask\n    plt.subplot(num_samples, 3, 3 * i + 3)\n    plt.imshow(images[i, 0], cmap=\"gray\")\n    plt.imshow(preds[i], cmap=\"jet\", alpha=0.5)\n    plt.title(\"Predicted Mask\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T22:48:41.792057Z","iopub.execute_input":"2025-04-29T22:48:41.792890Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Testing \n","metadata":{}},{"cell_type":"code","source":"import nibabel as nib\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport os\n\n# Define paths\nnew_file_path = \"/kaggle/input/zendo-fed-bca/FedBCa/Center2/T2WI/01.nii/001 ZHENG CHENG.nii\"  # Adjust this path to your new file\n# model_path = \"bladder_tumor_segmentation_model.pth\"  # Path to your saved model weights\n\n# Function to load a NIfTI file\ndef load_nifti(file_path):\n    nifti = nib.load(file_path)\n    return nifti.get_fdata()\n\n# Load and preprocess the new T2WI file\nnew_image = load_nifti(new_file_path)\nnew_image = (new_image - new_image.min()) / (new_image.max() - new_image.min() + 1e-8)  # Normalize to [0, 1]\n\n# Define a dataset class for the new file (without annotations)\nclass TestDataset(Dataset):\n    def __init__(self, image, resize_transform):\n        self.image = image\n        self.resize = resize_transform\n        self.slices = []\n        # Collect all slices (we'll predict on all slices since we don't have annotations)\n        for z in range(image.shape[2]):\n            self.slices.append(z)\n\n    def __len__(self):\n        return len(self.slices)\n\n    def __getitem__(self, idx):\n        z = self.slices[idx]\n        img_slice = self.image[:, :, z]\n        img_slice = torch.FloatTensor(img_slice).unsqueeze(0)  # Shape: [1, H, W]\n        img_slice = self.resize(img_slice)  # Resize to 256x256\n        return img_slice, z  # Return the slice and its z-index\n\n# Define resize transform\nresize = transforms.Resize((256, 256))\n\n# Create test dataset and dataloader\ntest_dataset = TestDataset(new_image, resize)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n\n# Load the trained model\n# !pip install segmentation-models-pytorch\nimport segmentation_models_pytorch as smp\n\nmodel = smp.Unet(\n    encoder_name=\"resnet34\",\n    encoder_weights=None,  # No pretrained weights since we're loading our own\n    in_channels=1,\n    classes=2,\n)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Load the saved model weights\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()\n\n# Make predictions on the new file\npredicted_masks = []\nslice_indices = []\nwith torch.no_grad():\n    for images_batch, z_indices in test_loader:\n        images_batch = images_batch.to(device)\n        outputs = model(images_batch)\n        preds = torch.argmax(outputs, dim=1)  # Convert logits to class predictions\n        predicted_masks.append(preds.cpu())\n        slice_indices.extend(z_indices.tolist())\n\n# Concatenate all predicted masks\npredicted_masks = torch.cat(predicted_masks, dim=0)  # Shape: [num_slices, 256, 256]\n\n# Visualize predictions for slices with predicted tumors\ntumor_slices = [i for i, idx in enumerate(slice_indices) if predicted_masks[i].sum() > 0]\nnum_samples = min(4, len(tumor_slices))  # Visualize up to 4 slices with predicted tumors\n\nif num_samples == 0:\n    print(\"No tumors detected in the new file. Showing a few slices anyway.\")\n    tumor_slices = [len(slice_indices) // 4, len(slice_indices) // 2, 3 * len(slice_indices) // 4]\n    num_samples = 3\n\nplt.figure(figsize=(10, 4 * num_samples))\nfor i in range(num_samples):\n    slice_idx = tumor_slices[i]\n    z = slice_indices[slice_idx]\n    \n    # T2WI image\n    plt.subplot(num_samples, 2, 2 * i + 1)\n    img_slice = test_dataset.image[:, :, z]\n    img_slice = torch.FloatTensor(img_slice).unsqueeze(0)\n    img_slice = resize(img_slice).squeeze(0).numpy()\n    plt.imshow(img_slice, cmap=\"gray\")\n    plt.title(f\"T2WI Slice {z}\")\n    plt.axis(\"off\")\n    \n    # Predicted mask\n    plt.subplot(num_samples, 2, 2 * i + 2)\n    plt.imshow(img_slice, cmap=\"gray\")\n    plt.imshow(predicted_masks[slice_idx], cmap=\"jet\", alpha=0.5)\n    plt.title(f\"Predicted Tumor Mask - Slice {z}\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Classification \n","metadata":{}},{"cell_type":"code","source":"import nibabel as nib\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nfrom torchvision import transforms, models\nimport os\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# Define paths to your dataset\ndata_dir = \"/kaggle/input/all-zendo-dataset/DATA/\"\nt2wi_dir = os.path.join(data_dir, \"T2WI\")\ncsv_path = \"/kaggle/input/all-zendo-dataset/DATA/all_centers_combined.csv\"\n\n# Load the CSV file\ndf = pd.read_csv(csv_path)\ndf['image_name'] = df['image_name'].str.replace('.nii.gz', '.nii')\nprint(\"CSV Columns:\", df.columns)\nprint(\"First few rows of CSV:\")\nprint(df.head())\n\n# Function to load a NIfTI file\ndef load_nifti(file_path):\n    nifti = nib.load(file_path)\n    return nifti.get_fdata()\n\n# Pair T2WI images with their labels from the CSV\nimage_label_pairs = []\nfor t2wi_subfolder in os.listdir(t2wi_dir):\n    subfolder_path = os.path.join(t2wi_dir, t2wi_subfolder)\n    if not os.path.isdir(subfolder_path):\n        continue\n    t2wi_files = [f for f in os.listdir(subfolder_path) if f.endswith(\".nii\")]\n    if t2wi_files:\n        t2wi_path = os.path.join(subfolder_path, t2wi_files[0])\n        t2wi_id = t2wi_subfolder.split(\".\")[0]\n        label_row = df[df['image_name'].str.replace('.nii', '') == t2wi_id]\n        if not label_row.empty:\n            label = label_row['label'].iloc[0]\n            image_label_pairs.append((t2wi_path, label))\n        else:\n            print(f\"No label found for T2WI ID {t2wi_id}\")\n\nprint(\"Paired T2WI-Label pairs:\", len(image_label_pairs))\nfor t2wi_path, label in image_label_pairs[:5]:\n    print(f\"T2WI: {t2wi_path}, Label: {label}\")\n\n# Compute class weights to handle imbalance\nlabels = [pair[1] for pair in image_label_pairs]\nnmbic_count = labels.count(0)\nmbic_count = labels.count(1)\ntotal = nmbic_count + mbic_count\nclass_weights = torch.tensor([total / (2 * nmbic_count), total / (2 * mbic_count)]).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\nprint(f\"Class Weights (NMBIC, MBIC): {class_weights}\")\n\n# Define data augmentation and transforms\ntrain_transform = A.Compose([\n    A.Resize(256, 256),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.Rotate(limit=30, p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.Normalize(mean=0.5, std=0.5),\n    ToTensorV2(),\n])\n\nval_transform = A.Compose([\n    A.Resize(256, 256),\n    A.Normalize(mean=0.5, std=0.5),\n    ToTensorV2(),\n])\n\n# Custom Dataset class for classification\nclass BladderCancerDataset(Dataset):\n    def __init__(self, image_label_pairs, transform=None):\n        self.image_label_pairs = image_label_pairs\n        self.transform = transform\n        self.slices = []\n        for idx, (img_path, label) in enumerate(self.image_label_pairs):\n            try:\n                img = load_nifti(img_path)\n                img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n                middle_slice = img.shape[2] // 2\n                self.slices.append((idx, middle_slice, img, label))\n            except Exception as e:\n                print(f\"Error loading {img_path}: {e}\")\n\n    def __len__(self):\n        return len(self.slices)\n\n    def __getitem__(self, idx):\n        img_idx, z, img, label = self.slices[idx]\n        img_slice = img[:, :, z]\n        if self.transform:\n            augmented = self.transform(image=img_slice)\n            img_slice = augmented['image']\n        else:\n            img_slice = torch.FloatTensor(img_slice).unsqueeze(0)\n        label = torch.LongTensor([label])\n        return img_slice, label\n\n    def get_pair_index(self, idx):\n        return self.slices[idx][0]\n\n# Create datasets with transforms\ntrain_dataset = BladderCancerDataset(image_label_pairs, transform=train_transform)\nval_dataset = BladderCancerDataset(image_label_pairs, transform=val_transform)\n\n# Split at the pair level\npair_indices = list(range(len(image_label_pairs)))\ntrain_pair_indices, val_pair_indices = train_test_split(pair_indices, test_size=0.2, random_state=42)\ntrain_indices, val_indices = [], []\nfor slice_idx in range(len(train_dataset)):\n    pair_idx = train_dataset.get_pair_index(slice_idx)\n    if pair_idx in train_pair_indices:\n        train_indices.append(slice_idx)\n    elif pair_idx in val_pair_indices:\n        val_indices.append(slice_idx)\n\n# Create data loaders\ntrain_sampler = SubsetRandomSampler(train_indices)\nval_sampler = SubsetRandomSampler(val_indices)\ntrain_loader = DataLoader(train_dataset, batch_size=8, sampler=train_sampler)\nval_loader = DataLoader(val_dataset, batch_size=8, sampler=val_sampler)\n\n# Use a pretrained ResNet50 model\nmodel = models.resnet50(pretrained=True)\nmodel.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\nmodel.fc = nn.Sequential(\n    nn.Linear(model.fc.in_features, 512),\n    nn.ReLU(),\n    nn.Dropout(0.6),\n    nn.Linear(512, 2)\n)\n\n# Move model to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Define loss, optimizer, and scheduler with weight decay\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n\n# Training loop with validation and early stopping\nnum_epochs = 100\ntrain_losses, val_losses = [], []\ntrain_accuracies, val_accuracies = [], []\nbest_val_loss = float('inf')\npatience = 10\nearly_stop_counter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    train_correct = 0\n    train_total = 0\n    for images_batch, labels_batch in train_loader:\n        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device).squeeze(1)\n        outputs = model(images_batch)\n        loss = criterion(outputs, labels_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        train_total += labels_batch.size(0)\n        train_correct += (predicted == labels_batch).sum().item()\n    train_loss /= len(train_loader)\n    train_accuracy = train_correct / train_total\n    train_losses.append(train_loss)\n    train_accuracies.append(train_accuracy)\n\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for images_batch, labels_batch in val_loader:\n            images_batch, labels_batch = images_batch.to(device), labels_batch.to(device).squeeze(1)\n            outputs = model(images_batch)\n            loss = criterion(outputs, labels_batch)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            val_total += labels_batch.size(0)\n            val_correct += (predicted == labels_batch).sum().item()\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels_batch.cpu().numpy())\n    val_loss /= len(val_loader)\n    val_accuracy = val_correct / val_total\n    val_losses.append(val_loss)\n    val_accuracies.append(val_accuracy)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n    \n    scheduler.step(val_loss)\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        early_stop_counter = 0\n        torch.save(model.state_dict(), \"best_model.pth\")\n    else:\n        early_stop_counter += 1\n        if early_stop_counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\n# Load the best model\nmodel.load_state_dict(torch.load(\"best_model.pth\"))\n\n# Plot training and validation accuracy/loss\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\nplt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Val Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.grid()\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\nplt.plot(range(1, len(val_losses) + 1), val_losses, label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.grid()\nplt.tight_layout()\nplt.show()\n\n# Generate and plot confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(6, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['NMBIC', 'MBIC'], yticklabels=['NMBIC', 'MBIC'])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Visualize predictions on validation set\nmodel.eval()\nval_iter = iter(val_loader)\nimages, labels = next(val_iter)\nimages, labels = images.to(device), labels.to(device).squeeze(1)\nwith torch.no_grad():\n    outputs = model(images)\n    _, preds = torch.max(outputs, 1)\nimages, labels, preds = images.cpu(), labels.cpu(), preds.cpu()\nnum_samples = min(15, len(images))\nplt.figure(figsize=(15, 4 * num_samples))\nfor i in range(num_samples):\n    plt.subplot(num_samples, 2, 2 * i + 1)\n    plt.imshow(images[i, 0], cmap=\"gray\")\n    plt.title(f\"Label: {'MBIC' if labels[i] == 1 else 'NMBIC'}\")\n    plt.axis(\"off\")\n    plt.subplot(num_samples, 2, 2 * i + 2)\n    plt.imshow(images[i, 0], cmap=\"gray\")\n    plt.title(f\"Predicted: {'MBIC' if preds[i] == 1 else 'NMBIC'}\")\n    plt.axis(\"off\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T23:13:25.318866Z","iopub.execute_input":"2025-04-29T23:13:25.319192Z","iopub.status.idle":"2025-04-29T23:13:27.906657Z","shell.execute_reply.started":"2025-04-29T23:13:25.319152Z","shell.execute_reply":"2025-04-29T23:13:27.904466Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"},{"name":"stdout","text":"CSV Columns: Index(['label', 'image_name', 'mask_name', 'Age (years)', 'Gender',\n       'Pathological T stage', 'Pathological grade',\n       'Type of patient's tumor number'],\n      dtype='object')\nFirst few rows of CSV:\n   label  image_name      mask_name  Age (years) Gender Pathological T stage  \\\n0      0  c1_001.nii  c1_001.nii.gz           67   Male                   Ta   \n1      0  c1_002.nii  c1_002.nii.gz           58   Male                   Ta   \n2      0  c1_003.nii  c1_003.nii.gz           75   Male                   Ta   \n3      0  c1_004.nii  c1_004.nii.gz           77   Male                   Ta   \n4      1  c1_005.nii  c1_005.nii.gz           67   Male                   T2   \n\n  Pathological grade Type of patient's tumor number  \n0                Low                         Single  \n1                Low                         Single  \n2                Low                         Single  \n3               High                         Single  \n4               High                         Single  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1940394994.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mt2wi_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfolder_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".nii\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt2wi_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mt2wi_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2wi_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":3}]}