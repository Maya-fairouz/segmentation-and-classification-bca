{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation and classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = -3\n",
      "       ***        \n",
      "    *********     \n",
      "   ***     ***    \n",
      "  **         **   \n",
      " **           **  \n",
      " **           **  \n",
      " *             *  \n",
      "**             ** \n",
      "**             ** \n",
      "**             ** \n",
      " *             *  \n",
      " **           **  \n",
      " **           **  \n",
      "  **         **   \n",
      "   ***     ***    \n",
      "    *********     \n",
      "       ***        \n",
      "                  \n",
      "\n",
      "\n",
      "z = -2\n",
      "      *****       \n",
      "    **** ****     \n",
      "   **       **    \n",
      "  **         **   \n",
      " **           **  \n",
      " *             *  \n",
      "**             ** \n",
      "**             ** \n",
      "*               * \n",
      "**             ** \n",
      "**             ** \n",
      " *             *  \n",
      " **           **  \n",
      "  **         **   \n",
      "   **       **    \n",
      "    **** ****     \n",
      "      *****       \n",
      "                  \n",
      "\n",
      "\n",
      "z = -1\n",
      "     *******      \n",
      "   ****   ****    \n",
      "  **         **   \n",
      " **           **  \n",
      " *             *  \n",
      "**             ** \n",
      "**             ** \n",
      "*               * \n",
      "*               * \n",
      "*               * \n",
      "**             ** \n",
      "**             ** \n",
      " *             *  \n",
      " **           **  \n",
      "  **         **   \n",
      "   ****   ****    \n",
      "     *******      \n",
      "                  \n",
      "\n",
      "\n",
      "z = 0\n",
      "     *******      \n",
      "   ***     ***    \n",
      "  **         **   \n",
      " **           **  \n",
      " *             *  \n",
      "**             ** \n",
      "*               * \n",
      "*               * \n",
      "*               * \n",
      "*               * \n",
      "*               * \n",
      "**             ** \n",
      " *             *  \n",
      " **           **  \n",
      "  **         **   \n",
      "   ***     ***    \n",
      "     *******      \n",
      "                  \n",
      "\n",
      "\n",
      "z = 1\n",
      "     *******      \n",
      "   ****   ****    \n",
      "  **         **   \n",
      " **           **  \n",
      " *             *  \n",
      "**             ** \n",
      "**             ** \n",
      "*               * \n",
      "*               * \n",
      "*               * \n",
      "**             ** \n",
      "**             ** \n",
      " *             *  \n",
      " **           **  \n",
      "  **         **   \n",
      "   ****   ****    \n",
      "     *******      \n",
      "                  \n",
      "\n",
      "\n",
      "z = 2\n",
      "      *****       \n",
      "    **** ****     \n",
      "   **       **    \n",
      "  **         **   \n",
      " **           **  \n",
      " *             *  \n",
      "**             ** \n",
      "**             ** \n",
      "*               * \n",
      "**             ** \n",
      "**             ** \n",
      " *             *  \n",
      " **           **  \n",
      "  **         **   \n",
      "   **       **    \n",
      "    **** ****     \n",
      "      *****       \n",
      "                  \n",
      "\n",
      "\n",
      "z = 3\n",
      "       ***        \n",
      "    *********     \n",
      "   ***     ***    \n",
      "  **         **   \n",
      " **           **  \n",
      " **           **  \n",
      " *             *  \n",
      "**             ** \n",
      "**             ** \n",
      "**             ** \n",
      " *             *  \n",
      " **           **  \n",
      " **           **  \n",
      "  **         **   \n",
      "   ***     ***    \n",
      "    *********     \n",
      "       ***        \n",
      "                  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def ascii_circle_3d(radius=10, z_levels=8, char='o'):\n",
    "    width = radius * 2 + 2\n",
    "    height = radius * 2 + 2\n",
    "    for z in range(z_levels):\n",
    "        print(f\"z = {z - z_levels//2}\")\n",
    "        for y in range(height):\n",
    "            row = \"\"\n",
    "            for x in range(width):\n",
    "                # Project the circle in 3D: x^2 + y^2 + z^2 = r^2\n",
    "                dx = x - radius\n",
    "                dy = y - radius\n",
    "                dz = z - z_levels//2\n",
    "                dist = math.sqrt(dx*dx + dy*dy + dz*dz)\n",
    "                if abs(dist - radius) < 0.7:\n",
    "                    row += char\n",
    "                else:\n",
    "                    row += \" \"\n",
    "            print(row)\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Example usage:\n",
    "ascii_circle_3d(radius=8, z_levels=7, char='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-29T22:28:53.639676Z",
     "iopub.status.busy": "2025-04-29T22:28:53.639489Z",
     "iopub.status.idle": "2025-04-29T22:29:57.921264Z",
     "shell.execute_reply": "2025-04-29T22:29:57.919986Z",
     "shell.execute_reply.started": "2025-04-29T22:28:53.639657Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths to your dataset\n",
    "data_dir = \"/kaggle/input/zendo-fed-bca/FedBCa/Center1/\"  # Adjust this path to your dataset\n",
    "t2wi_dir = os.path.join(data_dir, \"T2WI\")\n",
    "annotation_dir = os.path.join(data_dir, \"Annotation\")\n",
    "\n",
    "# Function to load a NIfTI file\n",
    "def load_nifti(file_path):\n",
    "    nifti = nib.load(file_path)\n",
    "    return nifti.get_fdata()\n",
    "\n",
    "# Get all T2WI subfolders and pair with annotations\n",
    "image_mask_pairs = []\n",
    "annotation_files = [f for f in os.listdir(annotation_dir) if f.endswith(\".nii\")]\n",
    "for t2wi_subfolder in os.listdir(t2wi_dir):\n",
    "    subfolder_path = os.path.join(t2wi_dir, t2wi_subfolder)\n",
    "    if not os.path.isdir(subfolder_path):\n",
    "        continue\n",
    "    t2wi_files = [f for f in os.listdir(subfolder_path) if f.endswith(\".nii\")]\n",
    "    for t2wi_file in t2wi_files:\n",
    "        t2wi_path = os.path.join(subfolder_path, t2wi_file)\n",
    "        t2wi_id = t2wi_subfolder.split(\".\")[0]  # e.g., \"001\"\n",
    "        matching_annotation = None\n",
    "        for ann_file in annotation_files:\n",
    "            if t2wi_id in ann_file:  # Adjust pairing logic as needed\n",
    "                matching_annotation = ann_file\n",
    "                break\n",
    "        if matching_annotation:\n",
    "            annotation_path = os.path.join(annotation_dir, matching_annotation)\n",
    "            image_mask_pairs.append((t2wi_path, annotation_path))\n",
    "        else:\n",
    "            print(f\"No matching annotation found for T2WI subfolder {t2wi_subfolder}\")\n",
    "\n",
    "# Load images and masks, and find representative slices\n",
    "images, masks, identifiers, slice_indices = [], [], [], []\n",
    "for t2wi_path, annotation_path in image_mask_pairs:\n",
    "    try:\n",
    "        img = load_nifti(t2wi_path)\n",
    "        mask = load_nifti(annotation_path)\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "        mask = (mask > 0).astype(np.uint8)\n",
    "        tumor_slices = [i for i in range(mask.shape[2]) if mask[:, :, i].sum() > 0]\n",
    "        slice_idx = tumor_slices[len(tumor_slices) // 2] if tumor_slices else img.shape[2] // 2\n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "        identifiers.append(os.path.basename(t2wi_path))\n",
    "        slice_indices.append(slice_idx)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {t2wi_path} or {annotation_path}: {e}\")\n",
    "\n",
    "# Visualize all images in a single grid\n",
    "num_pairs = len(images)\n",
    "cols = 2  # Two columns per pair (T2WI and T2WI+mask)\n",
    "rows = num_pairs  # One row per image-mask pair\n",
    "plt.figure(figsize=(12, 5 * rows))\n",
    "for i in range(num_pairs):\n",
    "    # Plot T2WI slice\n",
    "    plt.subplot(rows, cols, 2 * i + 1)\n",
    "    plt.imshow(images[i][:, :, slice_indices[i]], cmap=\"gray\")\n",
    "    plt.title(f\"T2WI {identifiers[i]} - Slice {slice_indices[i]}\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Plot T2WI slice with mask overlaid\n",
    "    plt.subplot(rows, cols, 2 * i + 2)\n",
    "    plt.imshow(images[i][:, :, slice_indices[i]], cmap=\"gray\")\n",
    "    plt.imshow(masks[i][:, :, slice_indices[i]], cmap=\"jet\", alpha=0.5)\n",
    "    plt.title(f\"T2WI {identifiers[i]} - Slice {slice_indices[i]} with Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmenation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T22:48:41.792890Z",
     "iopub.status.busy": "2025-04-29T22:48:41.792057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install segmentation-models-pytorch\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define paths to your dataset\n",
    "data_dir = \"/kaggle/input/all-zendo-dataset/DATA/\"  # Matches your dataset path\n",
    "t2wi_dir = os.path.join(data_dir, \"T2WI\")\n",
    "annotation_dir = os.path.join(data_dir, \"Annotation\")\n",
    "\n",
    "# Function to load a NIfTI file\n",
    "def load_nifti(file_path):\n",
    "    nifti = nib.load(file_path)\n",
    "    return nifti.get_fdata()\n",
    "\n",
    "# Pair T2WI images with their annotations\n",
    "image_mask_pairs = []\n",
    "# First, collect all T2WI files\n",
    "t2wi_files_dict = {}\n",
    "for t2wi_subfolder in os.listdir(t2wi_dir):\n",
    "    subfolder_path = os.path.join(t2wi_dir, t2wi_subfolder)\n",
    "    if not os.path.isdir(subfolder_path):\n",
    "        continue\n",
    "    t2wi_files = [f for f in os.listdir(subfolder_path) if f.endswith(\".nii\")]\n",
    "    if t2wi_files:  # Ensure there's at least one .nii file\n",
    "        t2wi_path = os.path.join(subfolder_path, t2wi_files[0])  # Take the first .nii file\n",
    "        t2wi_id = t2wi_subfolder.split(\".\")[0]  # e.g., \"001\"\n",
    "        t2wi_files_dict[t2wi_id] = t2wi_path\n",
    "\n",
    "# Now, collect annotation files (handle both direct .nii files and subfolders)\n",
    "for item in os.listdir(annotation_dir):\n",
    "    item_path = os.path.join(annotation_dir, item)\n",
    "    if os.path.isdir(item_path):  # Case 1: Subfolder like \"001.nii\"\n",
    "        ann_files = [f for f in os.listdir(item_path) if f.endswith(\".nii\")]\n",
    "        if ann_files:\n",
    "            ann_path = os.path.join(item_path, ann_files[0])\n",
    "            ann_id = item.split(\".\")[0]  # e.g., \"001\"\n",
    "            if ann_id in t2wi_files_dict:  # Match with T2WI\n",
    "                image_mask_pairs.append((t2wi_files_dict[ann_id], ann_path))\n",
    "    elif item.endswith(\".nii\"):  # Case 2: Direct .nii file\n",
    "        ann_id = item.split(\".\")[0]  # e.g., \"062YUANHEQ\" -> need to extract the ID part\n",
    "        # Adjust this based on your naming convention\n",
    "        # Assuming the ID is the first 3 characters (e.g., \"062\" for \"062YUANHEQ\")\n",
    "        possible_id = ann_id[:3] if len(ann_id) >= 3 else ann_id\n",
    "        if possible_id in t2wi_files_dict:\n",
    "            ann_path = item_path\n",
    "            image_mask_pairs.append((t2wi_files_dict[possible_id], ann_path))\n",
    "\n",
    "# Print the pairs to verify\n",
    "print(\"Paired T2WI-Annotation files:\", len(image_mask_pairs))\n",
    "for t2wi_path, ann_path in image_mask_pairs[:5]:  # Show first 5 pairs\n",
    "    print(f\"T2WI: {t2wi_path}, Annotation: {ann_path}\")\n",
    "\n",
    "# Define resize transform\n",
    "resize = transforms.Resize((256, 256))\n",
    "\n",
    "# Custom Dataset class\n",
    "class BladderTumorDataset(Dataset):\n",
    "    def __init__(self, image_mask_pairs):\n",
    "        self.image_mask_pairs = image_mask_pairs\n",
    "        self.slices = []\n",
    "        for idx, (img_path, mask_path) in enumerate(self.image_mask_pairs):\n",
    "            try:\n",
    "                img = load_nifti(img_path)\n",
    "                mask = load_nifti(mask_path)\n",
    "                img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "                mask = (mask > 0).astype(np.uint8)\n",
    "                for z in range(mask.shape[2]):\n",
    "                    if mask[:, :, z].sum() > 0:\n",
    "                        self.slices.append((idx, z, img, mask))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading pair {img_path} and {mask_path}: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx, z, img, mask = self.slices[idx]\n",
    "        img_slice = img[:, :, z]\n",
    "        mask_slice = mask[:, :, z]\n",
    "        img_slice = torch.FloatTensor(img_slice).unsqueeze(0)\n",
    "        mask_slice = torch.LongTensor(mask_slice)\n",
    "        img_slice = resize(img_slice)\n",
    "        mask_slice = resize(mask_slice.unsqueeze(0)).squeeze(0)\n",
    "        return img_slice, mask_slice\n",
    "\n",
    "    def get_pair_index(self, idx):\n",
    "        return self.slices[idx][0]\n",
    "\n",
    "# Create dataset\n",
    "dataset = BladderTumorDataset(image_mask_pairs)\n",
    "\n",
    "# Split at the pair level\n",
    "pair_indices = list(range(len(image_mask_pairs)))\n",
    "train_pair_indices, val_pair_indices = train_test_split(pair_indices, test_size=0.2, random_state=42)\n",
    "train_indices, val_indices = [], []\n",
    "for slice_idx in range(len(dataset)):\n",
    "    pair_idx = dataset.get_pair_index(slice_idx)\n",
    "    if pair_idx in train_pair_indices:\n",
    "        train_indices.append(slice_idx)\n",
    "    elif pair_idx in val_pair_indices:\n",
    "        val_indices.append(slice_idx)\n",
    "\n",
    "# Create data loaders\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "train_loader = DataLoader(dataset, batch_size=8, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=8, sampler=val_sampler)\n",
    "\n",
    "# Model setup and training\n",
    "!pip install segmentation-models-pytorch\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.optim as optim\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=1,\n",
    "    classes=2,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = smp.losses.DiceLoss(mode=\"multiclass\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def dice_score(pred, target, smooth=1):\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    intersection = (pred * target).sum()\n",
    "    return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "\n",
    "# Training loop with validation\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images_batch, masks_batch in train_loader:\n",
    "        images_batch, masks_batch = images_batch.to(device), masks_batch.to(device)\n",
    "        outputs = model(images_batch)\n",
    "        loss = criterion(outputs, masks_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_dice = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images_batch, masks_batch in val_loader:\n",
    "            images_batch, masks_batch = images_batch.to(device), masks_batch.to(device)\n",
    "            outputs = model(images_batch)\n",
    "            loss = criterion(outputs, masks_batch)\n",
    "            val_loss += loss.item()\n",
    "            dice = dice_score(outputs, masks_batch)\n",
    "            val_dice += dice.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_dice /= len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}\")\n",
    "\n",
    "# Visualize predictions on validation set\n",
    "model.eval()\n",
    "val_iter = iter(val_loader)\n",
    "images, masks = next(val_iter)  # Get one batch from validation set\n",
    "images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(images)\n",
    "    preds = torch.argmax(preds, dim=1)  # Convert logits to class predictions\n",
    "\n",
    "# Move to CPU for visualization\n",
    "images, masks, preds = images.cpu(), masks.cpu(), preds.cpu()\n",
    "\n",
    "# Visualize the first 18 samples in the batch (or fewer if batch is smaller)\n",
    "num_samples = min(18, len(images))\n",
    "plt.figure(figsize=(15, 4 * num_samples))\n",
    "for i in range(num_samples):\n",
    "    # T2WI image\n",
    "    plt.subplot(num_samples, 3, 3 * i + 1)\n",
    "    plt.imshow(images[i, 0], cmap=\"gray\")\n",
    "    plt.title(\"T2WI Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Ground truth mask\n",
    "    plt.subplot(num_samples, 3, 3 * i + 2)\n",
    "    plt.imshow(images[i, 0], cmap=\"gray\")\n",
    "    plt.imshow(masks[i], cmap=\"jet\", alpha=0.5)\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Predicted mask\n",
    "    plt.subplot(num_samples, 3, 3 * i + 3)\n",
    "    plt.imshow(images[i, 0], cmap=\"gray\")\n",
    "    plt.imshow(preds[i], cmap=\"jet\", alpha=0.5)\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "new_file_path = \"/kaggle/input/zendo-fed-bca/FedBCa/Center2/T2WI/01.nii/001 ZHENG CHENG.nii\"  # Adjust this path to your new file\n",
    "# model_path = \"bladder_tumor_segmentation_model.pth\"  # Path to your saved model weights\n",
    "\n",
    "# Function to load a NIfTI file\n",
    "def load_nifti(file_path):\n",
    "    nifti = nib.load(file_path)\n",
    "    return nifti.get_fdata()\n",
    "\n",
    "# Load and preprocess the new T2WI file\n",
    "new_image = load_nifti(new_file_path)\n",
    "new_image = (new_image - new_image.min()) / (new_image.max() - new_image.min() + 1e-8)  # Normalize to [0, 1]\n",
    "\n",
    "# Define a dataset class for the new file (without annotations)\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image, resize_transform):\n",
    "        self.image = image\n",
    "        self.resize = resize_transform\n",
    "        self.slices = []\n",
    "        # Collect all slices (we'll predict on all slices since we don't have annotations)\n",
    "        for z in range(image.shape[2]):\n",
    "            self.slices.append(z)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        z = self.slices[idx]\n",
    "        img_slice = self.image[:, :, z]\n",
    "        img_slice = torch.FloatTensor(img_slice).unsqueeze(0)  # Shape: [1, H, W]\n",
    "        img_slice = self.resize(img_slice)  # Resize to 256x256\n",
    "        return img_slice, z  # Return the slice and its z-index\n",
    "\n",
    "# Define resize transform\n",
    "resize = transforms.Resize((256, 256))\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "test_dataset = TestDataset(new_image, resize)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Load the trained model\n",
    "# !pip install segmentation-models-pytorch\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=None,  # No pretrained weights since we're loading our own\n",
    "    in_channels=1,\n",
    "    classes=2,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# Make predictions on the new file\n",
    "predicted_masks = []\n",
    "slice_indices = []\n",
    "with torch.no_grad():\n",
    "    for images_batch, z_indices in test_loader:\n",
    "        images_batch = images_batch.to(device)\n",
    "        outputs = model(images_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)  # Convert logits to class predictions\n",
    "        predicted_masks.append(preds.cpu())\n",
    "        slice_indices.extend(z_indices.tolist())\n",
    "\n",
    "# Concatenate all predicted masks\n",
    "predicted_masks = torch.cat(predicted_masks, dim=0)  # Shape: [num_slices, 256, 256]\n",
    "\n",
    "# Visualize predictions for slices with predicted tumors\n",
    "tumor_slices = [i for i, idx in enumerate(slice_indices) if predicted_masks[i].sum() > 0]\n",
    "num_samples = min(4, len(tumor_slices))  # Visualize up to 4 slices with predicted tumors\n",
    "\n",
    "if num_samples == 0:\n",
    "    print(\"No tumors detected in the new file. Showing a few slices anyway.\")\n",
    "    tumor_slices = [len(slice_indices) // 4, len(slice_indices) // 2, 3 * len(slice_indices) // 4]\n",
    "    num_samples = 3\n",
    "\n",
    "plt.figure(figsize=(10, 4 * num_samples))\n",
    "for i in range(num_samples):\n",
    "    slice_idx = tumor_slices[i]\n",
    "    z = slice_indices[slice_idx]\n",
    "    \n",
    "    # T2WI image\n",
    "    plt.subplot(num_samples, 2, 2 * i + 1)\n",
    "    img_slice = test_dataset.image[:, :, z]\n",
    "    img_slice = torch.FloatTensor(img_slice).unsqueeze(0)\n",
    "    img_slice = resize(img_slice).squeeze(0).numpy()\n",
    "    plt.imshow(img_slice, cmap=\"gray\")\n",
    "    plt.title(f\"T2WI Slice {z}\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Predicted mask\n",
    "    plt.subplot(num_samples, 2, 2 * i + 2)\n",
    "    plt.imshow(img_slice, cmap=\"gray\")\n",
    "    plt.imshow(predicted_masks[slice_idx], cmap=\"jet\", alpha=0.5)\n",
    "    plt.title(f\"Predicted Tumor Mask - Slice {z}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T23:13:25.319192Z",
     "iopub.status.busy": "2025-04-29T23:13:25.318866Z",
     "iopub.status.idle": "2025-04-29T23:13:27.906657Z",
     "shell.execute_reply": "2025-04-29T23:13:27.904466Z",
     "shell.execute_reply.started": "2025-04-29T23:13:25.319152Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Columns: Index(['label', 'image_name', 'mask_name', 'Age (years)', 'Gender',\n",
      "       'Pathological T stage', 'Pathological grade',\n",
      "       'Type of patient's tumor number'],\n",
      "      dtype='object')\n",
      "First few rows of CSV:\n",
      "   label  image_name      mask_name  Age (years) Gender Pathological T stage  \\\n",
      "0      0  c1_001.nii  c1_001.nii.gz           67   Male                   Ta   \n",
      "1      0  c1_002.nii  c1_002.nii.gz           58   Male                   Ta   \n",
      "2      0  c1_003.nii  c1_003.nii.gz           75   Male                   Ta   \n",
      "3      0  c1_004.nii  c1_004.nii.gz           77   Male                   Ta   \n",
      "4      1  c1_005.nii  c1_005.nii.gz           67   Male                   T2   \n",
      "\n",
      "  Pathological grade Type of patient's tumor number  \n",
      "0                Low                         Single  \n",
      "1                Low                         Single  \n",
      "2                Low                         Single  \n",
      "3               High                         Single  \n",
      "4               High                         Single  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/1940394994.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mt2wi_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfolder_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".nii\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt2wi_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mt2wi_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2wi_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms, models\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Define paths to your dataset\n",
    "data_dir = \"/kaggle/input/all-zendo-dataset/DATA/\"\n",
    "t2wi_dir = os.path.join(data_dir, \"T2WI\")\n",
    "csv_path = \"/kaggle/input/all-zendo-dataset/DATA/all_centers_combined.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "df['image_name'] = df['image_name'].str.replace('.nii.gz', '.nii')\n",
    "print(\"CSV Columns:\", df.columns)\n",
    "print(\"First few rows of CSV:\")\n",
    "print(df.head())\n",
    "\n",
    "# Function to load a NIfTI file\n",
    "def load_nifti(file_path):\n",
    "    nifti = nib.load(file_path)\n",
    "    return nifti.get_fdata()\n",
    "\n",
    "# Pair T2WI images with their labels from the CSV\n",
    "image_label_pairs = []\n",
    "for t2wi_subfolder in os.listdir(t2wi_dir):\n",
    "    subfolder_path = os.path.join(t2wi_dir, t2wi_subfolder)\n",
    "    if not os.path.isdir(subfolder_path):\n",
    "        continue\n",
    "    t2wi_files = [f for f in os.listdir(subfolder_path) if f.endswith(\".nii\")]\n",
    "    if t2wi_files:\n",
    "        t2wi_path = os.path.join(subfolder_path, t2wi_files[0])\n",
    "        t2wi_id = t2wi_subfolder.split(\".\")[0]\n",
    "        label_row = df[df['image_name'].str.replace('.nii', '') == t2wi_id]\n",
    "        if not label_row.empty:\n",
    "            label = label_row['label'].iloc[0]\n",
    "            image_label_pairs.append((t2wi_path, label))\n",
    "        else:\n",
    "            print(f\"No label found for T2WI ID {t2wi_id}\")\n",
    "\n",
    "print(\"Paired T2WI-Label pairs:\", len(image_label_pairs))\n",
    "for t2wi_path, label in image_label_pairs[:5]:\n",
    "    print(f\"T2WI: {t2wi_path}, Label: {label}\")\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "labels = [pair[1] for pair in image_label_pairs]\n",
    "nmbic_count = labels.count(0)\n",
    "mbic_count = labels.count(1)\n",
    "total = nmbic_count + mbic_count\n",
    "class_weights = torch.tensor([total / (2 * nmbic_count), total / (2 * mbic_count)]).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(f\"Class Weights (NMBIC, MBIC): {class_weights}\")\n",
    "\n",
    "# Define data augmentation and transforms\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=30, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Normalize(mean=0.5, std=0.5),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=0.5, std=0.5),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Custom Dataset class for classification\n",
    "class BladderCancerDataset(Dataset):\n",
    "    def __init__(self, image_label_pairs, transform=None):\n",
    "        self.image_label_pairs = image_label_pairs\n",
    "        self.transform = transform\n",
    "        self.slices = []\n",
    "        for idx, (img_path, label) in enumerate(self.image_label_pairs):\n",
    "            try:\n",
    "                img = load_nifti(img_path)\n",
    "                img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "                middle_slice = img.shape[2] // 2\n",
    "                self.slices.append((idx, middle_slice, img, label))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx, z, img, label = self.slices[idx]\n",
    "        img_slice = img[:, :, z]\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img_slice)\n",
    "            img_slice = augmented['image']\n",
    "        else:\n",
    "            img_slice = torch.FloatTensor(img_slice).unsqueeze(0)\n",
    "        label = torch.LongTensor([label])\n",
    "        return img_slice, label\n",
    "\n",
    "    def get_pair_index(self, idx):\n",
    "        return self.slices[idx][0]\n",
    "\n",
    "# Create datasets with transforms\n",
    "train_dataset = BladderCancerDataset(image_label_pairs, transform=train_transform)\n",
    "val_dataset = BladderCancerDataset(image_label_pairs, transform=val_transform)\n",
    "\n",
    "# Split at the pair level\n",
    "pair_indices = list(range(len(image_label_pairs)))\n",
    "train_pair_indices, val_pair_indices = train_test_split(pair_indices, test_size=0.2, random_state=42)\n",
    "train_indices, val_indices = [], []\n",
    "for slice_idx in range(len(train_dataset)):\n",
    "    pair_idx = train_dataset.get_pair_index(slice_idx)\n",
    "    if pair_idx in train_pair_indices:\n",
    "        train_indices.append(slice_idx)\n",
    "    elif pair_idx in val_pair_indices:\n",
    "        val_indices.append(slice_idx)\n",
    "\n",
    "# Create data loaders\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, sampler=val_sampler)\n",
    "\n",
    "# Use a pretrained ResNet50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.6),\n",
    "    nn.Linear(512, 2)\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss, optimizer, and scheduler with weight decay\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "# Training loop with validation and early stopping\n",
    "num_epochs = 100\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for images_batch, labels_batch in train_loader:\n",
    "        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device).squeeze(1)\n",
    "        outputs = model(images_batch)\n",
    "        loss = criterion(outputs, labels_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels_batch.size(0)\n",
    "        train_correct += (predicted == labels_batch).sum().item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = train_correct / train_total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images_batch, labels_batch in val_loader:\n",
    "            images_batch, labels_batch = images_batch.to(device), labels_batch.to(device).squeeze(1)\n",
    "            outputs = model(images_batch)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels_batch.size(0)\n",
    "            val_correct += (predicted == labels_batch).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels_batch.cpu().numpy())\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "# Plot training and validation accuracy/loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate and plot confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['NMBIC', 'MBIC'], yticklabels=['NMBIC', 'MBIC'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Visualize predictions on validation set\n",
    "model.eval()\n",
    "val_iter = iter(val_loader)\n",
    "images, labels = next(val_iter)\n",
    "images, labels = images.to(device), labels.to(device).squeeze(1)\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "images, labels, preds = images.cpu(), labels.cpu(), preds.cpu()\n",
    "num_samples = min(15, len(images))\n",
    "plt.figure(figsize=(15, 4 * num_samples))\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(num_samples, 2, 2 * i + 1)\n",
    "    plt.imshow(images[i, 0], cmap=\"gray\")\n",
    "    plt.title(f\"Label: {'MBIC' if labels[i] == 1 else 'NMBIC'}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(num_samples, 2, 2 * i + 2)\n",
    "    plt.imshow(images[i, 0], cmap=\"gray\")\n",
    "    plt.title(f\"Predicted: {'MBIC' if preds[i] == 1 else 'NMBIC'}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7229200,
     "sourceId": 11526622,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7269824,
     "sourceId": 11593193,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
