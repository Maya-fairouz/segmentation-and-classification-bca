{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kjzfhkjzfhkzfjbn\n"
     ]
    }
   ],
   "source": [
    "print(\"kjzfhkjzfhkzfjbn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import nibabel as nib\n",
    "from scipy import ndimage\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Define paths to your dataset\n",
    "data_dir = \"/kaggle/input/all-zendo-dataset/DATA\"\n",
    "t2wi_dir = os.path.join(data_dir, \"T2WI\")\n",
    "csv_path = \"/kaggle/input/all-zendo-dataset/DATA/all_centers_combined.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "df['image_name'] = df['image_name'].str.replace('.nii.gz', '.nii')\n",
    "\n",
    "# Helper functions for preprocessing\n",
    "def read_nifti_file(filepath):\n",
    "    scan = nib.load(filepath).get_fdata()\n",
    "    return scan\n",
    "\n",
    "def normalize(volume):\n",
    "    min_hu, max_hu = -1000, 400\n",
    "    volume = np.clip(volume, min_hu, max_hu)\n",
    "    volume = (volume - min_hu) / (max_hu - min_hu)\n",
    "    return volume.astype(\"float32\")\n",
    "\n",
    "def resize_volume(img):\n",
    "    desired_depth, desired_width, desired_height = 64, 128, 128\n",
    "    current_depth, current_width, current_height = img.shape[-1], img.shape[0], img.shape[1]\n",
    "    depth_factor = current_depth / desired_depth\n",
    "    width_factor = current_width / desired_width\n",
    "    height_factor = current_height / desired_height\n",
    "    img = ndimage.rotate(img, 90, reshape=False)\n",
    "    img = ndimage.zoom(img,\n",
    "                       (1/width_factor, 1/height_factor, 1/depth_factor),\n",
    "                       order=1)\n",
    "    return img\n",
    "\n",
    "def process_scan(path):\n",
    "    volume = read_nifti_file(path)\n",
    "    volume = normalize(volume)\n",
    "    return resize_volume(volume)\n",
    "\n",
    "# Load and pair T2WI images with labels\n",
    "nmbic_scan_paths = []\n",
    "mbic_scan_paths = []\n",
    "for subfolder in os.listdir(t2wi_dir):\n",
    "    folder = os.path.join(t2wi_dir, subfolder)\n",
    "    if not os.path.isdir(folder):\n",
    "        continue\n",
    "    files = [f for f in os.listdir(folder) if f.endswith(\".nii\")]\n",
    "    if not files:\n",
    "        continue\n",
    "    path = os.path.join(folder, files[0])\n",
    "    img_id = subfolder.split(\".\")[0]\n",
    "    row = df[df['image_name'].str.replace('.nii', '') == img_id]\n",
    "    if row.empty:\n",
    "        print(f\"No label found for T2WI ID {img_id}\")\n",
    "        continue\n",
    "    label = int(row['label'].iloc[0])\n",
    "    if label == 0:\n",
    "        nmbic_scan_paths.append(path)\n",
    "    else:\n",
    "        mbic_scan_paths.append(path)\n",
    "\n",
    "print(f\"NMBIC scans: {len(nmbic_scan_paths)}\")\n",
    "print(f\"MBIC scans: {len(mbic_scan_paths)}\")\n",
    "\n",
    "# Process scans\n",
    "nmbic_scans = np.array([process_scan(p) for p in nmbic_scan_paths])\n",
    "mbic_scans = np.array([process_scan(p) for p in mbic_scan_paths])\n",
    "\n",
    "# Assign labels\n",
    "nmbic_labels = np.zeros(len(nmbic_scans), dtype=int)\n",
    "mbic_labels = np.ones(len(mbic_scans), dtype=int)\n",
    "\n",
    "# Split data into training, validation, and test (60-20-20)\n",
    "def split_data(X, y, train_ratio=0.6, val_ratio=0.2):\n",
    "    n_total = len(X)\n",
    "    n_train = int(train_ratio * n_total)\n",
    "    n_val = int(val_ratio * n_total)\n",
    "    return X[:n_train], y[:n_train], X[n_train:n_train+n_val], y[n_train:n_train+n_val], X[n_train+n_val:], y[n_train+n_val:]\n",
    "\n",
    "x_tr_n, y_tr_n, x_val_n, y_val_n, x_test_n, y_test_n = split_data(nmbic_scans, nmbic_labels)\n",
    "x_tr_p, y_tr_p, x_val_p, y_val_p, x_test_p, y_test_p = split_data(mbic_scans, mbic_labels)\n",
    "\n",
    "x_train = np.concatenate((x_tr_p, x_tr_n), axis=0)\n",
    "y_train = np.concatenate((y_tr_p, y_tr_n), axis=0)\n",
    "x_val = np.concatenate((x_val_p, x_val_n), axis=0)\n",
    "y_val = np.concatenate((y_val_p, y_val_n), axis=0)\n",
    "x_test = np.concatenate((x_test_p, x_test_n), axis=0)\n",
    "y_test = np.concatenate((y_test_p, y_test_n), axis=0)\n",
    "\n",
    "print(f\"Number of samples in train, validation, and test are {x_train.shape[0]}, {x_val.shape[0]}, and {x_test.shape[0]}.\")\n",
    "\n",
    "# Address class imbalance via class weights\n",
    "classes = np.unique(y_train)\n",
    "cw = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, cw))\n",
    "print(\"Computed class weights:\", class_weight_dict)\n",
    "\n",
    "# Data augmentation\n",
    "def rotate(volume):\n",
    "    def scipy_rotate(vol):\n",
    "        angles = [-20, -10, -5, 5, 10, 20]\n",
    "        angle = np.random.choice(angles)\n",
    "        vol = ndimage.rotate(vol, angle, reshape=False)\n",
    "        vol = np.clip(vol, 0, 1)\n",
    "        return vol\n",
    "    return tf.numpy_function(scipy_rotate, [volume], tf.float32)\n",
    "\n",
    "def train_preprocessing(volume, label):\n",
    "    volume = rotate(volume)\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "\n",
    "def validation_preprocessing(volume, label):\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 2\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(len(x_train))\n",
    "    .map(train_preprocessing, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "validation_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    .map(validation_preprocessing, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    .map(validation_preprocessing, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "# place of the model \n",
    "\n",
    "\n",
    "# Evaluate on the validation set\n",
    "metrics = model.evaluate(validation_dataset, verbose=0)\n",
    "val_loss, val_accuracy, val_precision, val_recall, val_auc = metrics\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "metrics = model.evaluate(test_dataset, verbose=0)\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_auc = metrics\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Plot training and validation metrics\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy'); plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(history.history['recall'], label='Training Recall')\n",
    "plt.plot(history.history['val_recall'], label='Validation Recall')\n",
    "plt.title('Recall'); plt.xlabel('Epoch'); plt.ylabel('Recall'); plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(history.history['auc'], label='Training AUC')\n",
    "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "plt.title('AUC'); plt.xlabel('Epoch'); plt.ylabel('AUC'); plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics.png')\n",
    "plt.show()\n",
    "\n",
    "# Collect true labels and predictions for validation set\n",
    "y_val_list = np.array([y.numpy() for _, y in validation_dataset.unbatch()]).astype(int)\n",
    "y_val_pred = model.predict(validation_dataset)\n",
    "y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
    "\n",
    "# DataFrame of validation predictions\n",
    "val_df = pd.DataFrame({\n",
    "    'Sample Index': np.arange(len(y_val_list)),\n",
    "    'True Label': ['MBIC' if l==1 else 'NMBIC' for l in y_val_list],\n",
    "    'Pred Label': ['MBIC' if p==1 else 'NMBIC' for p in y_val_pred_binary.flatten()],\n",
    "    'Pred Prob': y_val_pred.flatten()\n",
    "})\n",
    "print(val_df)\n",
    "val_df.to_csv('validation_predictions.csv', index=False)\n",
    "\n",
    "# Classification report & confusion matrix for validation set\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val_list, y_val_pred_binary, target_names=[\"NMBIC\",\"MBIC\"]))\n",
    "cm = confusion_matrix(y_val_list, y_val_pred_binary)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['NMBIC','MBIC'], yticklabels=['NMBIC','MBIC'])\n",
    "plt.title('Validation Confusion Matrix (threshold=0.5)')\n",
    "plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Threshold optimization for validation set\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val_list, y_val_pred)\n",
    "opt_thr_f1 = thresholds[np.argmax(2*(precisions*recalls)/(precisions+recalls+1e-8))]\n",
    "print(f\"Validation Optimal threshold (max F1): {opt_thr_f1:.2f}\")\n",
    "y_val_pred_opt = (y_val_pred > opt_thr_f1).astype(int)\n",
    "print(\"Validation Classification Report (Optimal Threshold):\")\n",
    "print(classification_report(y_val_list, y_val_pred_opt, target_names=[\"NMBIC\",\"MBIC\"]))\n",
    "cm_opt = confusion_matrix(y_val_list, y_val_pred_opt)\n",
    "sns.heatmap(cm_opt, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['NMBIC','MBIC'], yticklabels=['NMBIC','MBIC'])\n",
    "plt.title(f'Validation Confusion Matrix (threshold={opt_thr_f1:.2f})')\n",
    "plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Collect true labels and predictions for test set\n",
    "y_test_list = np.array([y.numpy() for _, y in test_dataset.unbatch()]).astype(int)\n",
    "y_test_pred = model.predict(test_dataset)\n",
    "y_test_pred_binary = (y_test_pred > 0.5).astype(int)\n",
    "\n",
    "# DataFrame of test predictions\n",
    "test_df = pd.DataFrame({\n",
    "    'Sample Index': np.arange(len(y_test_list)),\n",
    "    'True Label': ['MBIC' if l==1 else 'NMBIC' for l in y_test_list],\n",
    "    'Pred Label': ['MBIC' if p==1 else 'NMBIC' for p in y_test_pred_binary.flatten()],\n",
    "    'Pred Prob': y_test_pred.flatten()\n",
    "})\n",
    "print(test_df)\n",
    "test_df.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "# Classification report & confusion matrix for test set\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test_list, y_test_pred_binary, target_names=[\"NMBIC\",\"MBIC\"]))\n",
    "cm = confusion_matrix(y_test_list, y_test_pred_binary)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['NMBIC','MBIC'], yticklabels=['NMBIC','MBIC'])\n",
    "plt.title('Test Confusion Matrix (threshold=0.5)')\n",
    "plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Threshold optimization for test set\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test_list, y_test_pred)\n",
    "opt_thr_f1 = thresholds[np.argmax(2*(precisions*recalls)/(precisions+recalls+1e-8))]\n",
    "print(f\"Test Optimal threshold (max F1): {opt_thr_f1:.2f}\")\n",
    "y_test_pred_opt = (y_test_pred > opt_thr_f1).astype(int)\n",
    "print(\"Test Classification Report (Optimal Threshold):\")\n",
    "print(classification_report(y_test_list, y_test_pred_opt, target_names=[\"NMBIC\",\"MBIC\"]))\n",
    "cm_opt = confusion_matrix(y_test_list, y_test_pred_opt)\n",
    "sns.heatmap(cm_opt, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['NMBIC','MBIC'], yticklabels=['NMBIC','MBIC'])\n",
    "plt.title(f'Test Confusion Matrix (threshold={opt_thr_f1:.2f})')\n",
    "plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "model.save('cnn_bladder_massive.keras')\n",
    "print(\"Model saved as 'cnn_bladder_massive.keras'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
